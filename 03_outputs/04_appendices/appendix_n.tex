
\documentclass[11pt]{article}
\usepackage[margin=1cm]{geometry}
\usepackage{booktabs}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{url}
\urlstyle{same}
\setlength{\tabcolsep}{6pt}
\sloppy
\pagestyle{empty}

\begin{document}
\noindent\hspace*{1cm}\textbf{\large Appendix N. Study 2: LLM response length differences to experimental manipulation.}

\vspace{1em}

{\small
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{p{2.3cm} p{4cm} p{2.5cm} p{2.5cm} p{1.3cm} p{1.3cm} p{1.3cm}}
\toprule
\textbf{LLM Provider} & \textbf{Model Name} & \textbf{What/How Mean (SD)} & \textbf{Hobson's C. Mean (SD)} & \textbf{\textit{t}-value} & \textbf{\textit{p}-value} & \textbf{Cohen's \textit{d}} \\
\midrule
Qwen & QwQ 32B & 874.4 (759.7) & 585.6 (298.0) & 7.87 & \textless\ .001 & 0.38 \\
\addlinespace[0.7em]
Qwen & Qwen2.5 72B Instruct Turbo & 591.8 (279.2) & 371.6 (248.6) & 18.00 & \textless\ .001 & 0.87 \\
\addlinespace[0.7em]
\addlinespace[1.0em]
Anthropic & claude 3 haiku 20240307 & 248.1 (101.0) & 187.8 (99.1) & 12.33 & \textless\ .001 & 0.60 \\
\addlinespace[0.7em]
Anthropic & claude 3 opus 20240229 & 260.5 (81.8) & 214.4 (95.6) & 10.69 & \textless\ .001 & 0.52 \\
\addlinespace[0.7em]
Anthropic & claude sonnet 4 20250514 & 266.8 (74.1) & 194.3 (77.4) & 19.05 & \textless\ .001 & 0.92 \\
\addlinespace[0.7em]
\addlinespace[1.0em]
Deepseek-Ai & DeepSeek R1 & 572.5 (134.1) & 587.3 (161.3) & -1.86 & 0.063 & -0.09 \\
\addlinespace[0.7em]
Deepseek-Ai & DeepSeek R1 0528 tput & 941.5 (400.0) & 737.3 (353.9) & 8.75 & \textless\ .001 & 0.42 \\
\addlinespace[0.7em]
\addlinespace[1.0em]
Google & gemini 1.5 pro & 526.5 (216.0) & 308.1 (233.4) & 20.10 & \textless\ .001 & 0.97 \\
\addlinespace[0.7em]
Google & gemini 2.5 flash & 1110.2 (521.6) & 675.5 (507.6) & 17.82 & \textless\ .001 & 0.87 \\
\addlinespace[0.7em]
Google & gemma 2 27b it & 402.2 (162.7) & 280.0 (161.6) & 16.22 & \textless\ .001 & 0.79 \\
\addlinespace[0.7em]
Google & gemma 3n E4B it & 1224.9 (527.2) & 698.7 (498.0) & 21.84 & \textless\ .001 & 1.06 \\
\addlinespace[0.7em]
\addlinespace[1.0em]
Meta-Llama & Llama 3.3 70B Instruct Turbo & 568.9 (178.2) & 398.0 (192.2) & 19.84 & \textless\ .001 & 0.96 \\
\addlinespace[0.7em]
Meta-Llama & Llama 4 Scout 17B 16E Instruct & 552.0 (192.7) & 377.3 (204.6) & 17.53 & \textless\ .001 & 0.85 \\
\addlinespace[0.7em]
Meta-Llama & Meta Llama 3 8B Instruct Lite & 499.5 (167.7) & 355.6 (192.9) & 15.30 & \textless\ .001 & 0.74 \\
\addlinespace[0.7em]
\addlinespace[1.0em]
Mistral & magistral medium 2506 & 464.0 (414.1) & 541.2 (440.1) & -2.38 & 0.018 & -0.12 \\
\addlinespace[0.7em]
Mistral & mistral medium latest & 609.2 (210.0) & 432.4 (214.8) & 17.26 & \textless\ .001 & 0.84 \\
\addlinespace[0.7em]
Mistral & mistral small & 325.2 (137.4) & 244.0 (121.6) & 12.06 & \textless\ .001 & 0.58 \\
\addlinespace[0.7em]
\addlinespace[1.0em]
Openai & gpt 4.1 & 500.8 (216.6) & 272.3 (185.3) & 23.21 & \textless\ .001 & 1.12 \\
\addlinespace[0.7em]
Openai & gpt 4.1 mini & 374.9 (207.9) & 172.4 (158.3) & 20.86 & \textless\ .001 & 1.01 \\
\addlinespace[0.7em]
Openai & gpt 4.1 nano & 327.1 (169.1) & 153.7 (138.1) & 21.77 & \textless\ .001 & 1.05 \\
\addlinespace[0.7em]
Openai & o4 mini & 680.0 (282.3) & 515.1 (261.7) & 12.90 & \textless\ .001 & 0.62 \\
\addlinespace[0.7em]
\bottomrule
\end{tabular}

\vspace{1em}
\begin{center}
\begin{minipage}{16.2cm}
{\fontsize{10}{12}\selectfont
\textit{Note.} Hobson's C (Hobson's Choice) is the most closed-ended type of interrogative, and What/How is the most open-ended form of interrogative according to the taxonomy of interrogatives by Belnap \& Steel (1976). All LLM responses were tokenized using the same tokenizer (via the \texttt{tiktoken} library), making lengths comparable across models. \textit{t}-values are from paired t-tests; Cohen's \textit{d} quantifies the difference in response length between What/How and Hobsonâ€™s prompts, with positive values indicating longer responses to What/How.
}
\end{minipage}
\end{center}

\end{document}
