
@inproceedings{rottger_two_2022,
	address = {Seattle, United States},
	title = {Two {Contrasting} {Data} {Annotation} {Paradigms} for {Subjective} {NLP} {Tasks}},
	url = {https://aclanthology.org/2022.naacl-main.13},
	doi = {10.18653/v1/2022.naacl-main.13},
	abstract = {Labelled data is the foundation of most natural language processing tasks. However, labelling data is difﬁcult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss beneﬁts and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.},
	language = {en},
	urldate = {2025-07-16},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Röttger, Paul and Vidgen, Bertie and Hovy, Dirk and Pierrehumbert, Janet},
	year = {2022},
	file = {PDF:/Users/carolinewagner/Zotero/storage/NHKA83Z7/Rottger et al. - 2022 - Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks.pdf:application/pdf},
}

@article{laux_how_nodate,
	title = {How {Clear} {Rules} and {Higher} {Pay} {Increase} {Performance} in {Data} {Annotation} in the {AI} {Economy}},
	abstract = {The global surge in AI applications is transforming industries, leading to displacement and complementation of existing jobs, while also giving rise to new employment opportunities. Data annotation, encompassing the labelling of images or annotating of texts by human workers, crucially influences the quality of a dataset directly influences the quality of AI models trained on it. This paper delves into the economics of data annotation, with a specific focus on the impact of task instruction design (that is, the choice between rules and standards as theorised in law and economics) and monetary incentives on data quality and costs. An experimental study involving 307 data annotators examines six groups with varying task instructions (norms) and monetary incentives. Results reveal that annotators provided with clear rules exhibit higher accuracy rates, outperforming those with vague standards by 14\%. Similarly, annotators receiving an additional monetary incentive perform significantly better, with the highest accuracy rate recorded in the group working with both clear rules and incentives (87.5\% accuracy). In addition, our results show that rules are perceived as being more helpful by annotators than standards and reduce annotators’ difficulty in annotating images. These empirical findings underscore the double benefit of rule-based instructions on both data quality and worker wellbeing. Our research design allows us to reveal that, in our study, rules are more cost-efficient in increasing accuracy than monetary incentives. The paper contributes experimental insights to discussions on the economical, ethical, and legal considerations of AI technologies. Addressing policymakers and practitioners, we emphasise the need for a balanced approach in optimising data annotation processes for efficient and ethical AI development and usage.},
	language = {en},
	author = {Laux, Johann and Stephany, Fabian and Liefgreen, Alice},
	file = {PDF:/Users/carolinewagner/Zotero/storage/YLXTUCFD/Laux et al. - How Clear Rules and Higher Pay Increase Performance in Data Annotation in the AI Economy.pdf:application/pdf},
}

@article{cohen_kadosh_high_2015,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Conditioning, Classical, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Reflex, Startle},
	pages = {50--57},
}

@article{cohen_kadosh_high_2015-1,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Conditioning, Classical, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Reflex, Startle},
	pages = {50--57},
	file = {Accepted Version:/Users/carolinewagner/Zotero/storage/6YWXYKA9/Cohen Kadosh et al. - 2015 - High trait anxiety during adolescence interferes w.pdf:application/pdf},
}

@article{cohen_kadosh_high_2015-2,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Conditioning, Classical, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Reflex, Startle},
	pages = {50--57},
}

@article{cohen_kadosh_high_2015-3,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Conditioning, Classical, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Reflex, Startle},
	pages = {50--57},
	file = {Accepted Version:/Users/carolinewagner/Zotero/storage/K9C85MYC/Cohen Kadosh et al. - 2015 - High trait anxiety during adolescence interferes w.pdf:application/pdf},
}

@misc{laux_improving_2023,
	title = {Improving {Task} {Instructions} for {Data} {Annotators}: {How} {Clear} {Rules} and {Higher} {Pay} {Increase} {Performance} in {Data} {Annotation} in the {AI} {Economy}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Improving {Task} {Instructions} for {Data} {Annotators}},
	url = {https://arxiv.org/abs/2312.14565},
	doi = {10.48550/ARXIV.2312.14565},
	abstract = {The global surge in AI applications is transforming industries, leading to displacement and complementation of existing jobs, while also giving rise to new employment opportunities. Data annotation, encompassing the labelling of images or annotating of texts by human workers, crucially influences the quality of a dataset directly influences the quality of AI models trained on it. This paper delves into the economics of data annotation, with a specific focus on the impact of task instruction design (that is, the choice between rules and standards as theorised in law and economics) and monetary incentives on data quality and costs. An experimental study involving 307 data annotators examines six groups with varying task instructions (norms) and monetary incentives. Results reveal that annotators provided with clear rules exhibit higher accuracy rates, outperforming those with vague standards by 14\%. Similarly, annotators receiving an additional monetary incentive perform significantly better, with the highest accuracy rate recorded in the group working with both clear rules and incentives (87.5\% accuracy). In addition, our results show that rules are perceived as being more helpful by annotators than standards and reduce annotators' difficulty in annotating images. These empirical findings underscore the double benefit of rule-based instructions on both data quality and worker wellbeing. Our research design allows us to reveal that, in our study, rules are more cost-efficient in increasing accuracy than monetary incentives. The paper contributes experimental insights to discussions on the economical, ethical, and legal considerations of AI technologies. Addressing policymakers and practitioners, we emphasise the need for a balanced approach in optimising data annotation processes for efficient and ethical AI development and usage.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Laux, Johann and Stephany, Fabian and Liefgreen, Alice},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Applications (stat.AP), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Economics and business, General Economics (econ.GN)},
}

@book{grice_studies_1991,
	address = {Cambridge (Mass.) London},
	title = {Studies in the way of words},
	isbn = {978-0-674-85271-6},
	language = {eng},
	publisher = {Harvard university press},
	author = {Grice, H. Paul},
	year = {1991},
}

@article{kasirzadeh_conversation_2023,
	title = {In {Conversation} with {Artificial} {Intelligence}: {Aligning} language {Models} with {Human} {Values}},
	volume = {36},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2210-5433, 2210-5441},
	shorttitle = {In {Conversation} with {Artificial} {Intelligence}},
	url = {https://link.springer.com/10.1007/s13347-023-00606-x},
	doi = {10.1007/s13347-023-00606-x},
	abstract = {AbstractLarge-scale language technologies are increasingly used in various forms of communication with humans across different contexts. One particular use case for these technologies is conversational agents, which output natural language text in response to prompts and queries. This mode of engagement raises a number of social and ethical questions. For example, what does it mean to align conversational agents with human norms or values? Which norms or values should they be aligned with? And how can this be accomplished? In this paper, we propose a number of steps that help answer these questions. We start by developing a philosophical analysis of the building blocks of linguistic communication between conversational agents and human interlocutors. We then use this analysis to identify and formulate ideal norms of conversation that can govern successful linguistic communication between humans and conversational agents. Furthermore, we explore how these norms can be used to align conversational agents with human values across a range of different discursive domains. We conclude by discussing the practical implications of our proposal for the design of conversational agents that are aligned with these norms and values.},
	language = {en},
	number = {2},
	urldate = {2025-07-16},
	journal = {Philosophy \& Technology},
	author = {Kasirzadeh, Atoosa and Gabriel, Iason},
	month = jun,
	year = {2023},
	note = {Publisher: Springer Science and Business Media LLC},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/L4Y9WR3G/Kasirzadeh and Gabriel - 2023 - In Conversation with Artificial Intelligence Aligning language Models with Human Values.pdf:application/pdf},
}

@misc{kim_applying_2025,
	title = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}: {Design} {Insights} from a {Participatory} {Approach}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}},
	url = {https://arxiv.org/abs/2503.00858},
	doi = {10.48550/ARXIV.2503.00858},
	abstract = {While large language models (LLMs) are increasingly used to assist users in various tasks through natural language interactions, these interactions often fall short due to LLMs' limited ability to infer contextual nuances and user intentions, unlike humans. To address this challenge, we draw inspiration from the Gricean Maxims--human communication theory that suggests principles of effective communication--and aim to derive design insights for enhancing human-AI interactions (HAI). Through participatory design workshops with communication experts, designers, and end-users, we identified ways to apply these maxims across the stages of the HAI cycle. Our findings include reinterpreted maxims tailored to human-LLM contexts and nine actionable design considerations categorized by interaction stage. These insights provide a concrete framework for designing more cooperative and user-centered LLM-based systems, bridging theoretical foundations in communication with practical applications in HAI.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Kim, Yoonsu and Chin, Brandon and Son, Kihoon and Kim, Seoyoung and Kim, Juho},
	year = {2025},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Human-Computer Interaction (cs.HC)},
}

@misc{saad_gricean_2025,
	title = {Gricean {Norms} as a {Basis} for {Effective} {Collaboration}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2503.14484},
	doi = {10.48550/ARXIV.2503.14484},
	abstract = {Effective human-AI collaboration hinges not only on the AI agent's ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks -- common ground, relevance theory, and theory of mind -- into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent's pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Saad, Fardin and Murukannaiah, Pradeep K. and Singh, Munindar P.},
	year = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Computation and Language (cs.CL), Multiagent Systems (cs.MA)},
}

@inproceedings{krause_gricean_2024,
	address = {Tokyo, Japan},
	title = {The {Gricean} {Maxims} in {NLP} - {A} {Survey}},
	url = {https://aclanthology.org/2024.inlg-main.39},
	doi = {10.18653/v1/2024.inlg-main.39},
	urldate = {2025-07-16},
	booktitle = {Proceedings of the 17th {International} {Natural} {Language} {Generation} {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {Krause, Lea and Vossen, Piek T.J.M.},
	year = {2024},
	pages = {470--485},
}

@incollection{bloom_taxonomy_1986,
	address = {London},
	edition = {29. print},
	title = {Taxonomy of educational objectives.},
	isbn = {978-0-582-28010-6},
	shorttitle = {Taxonomy of educational objectives. 1},
	language = {eng},
	publisher = {Longman},
	author = {Bloom, Benjamin S.},
	year = {1986},
	note = {Num Pages: 207},
}

@misc{luo_enhanced_2025,
	title = {Enhanced {Bloom}'s {Educational} {Taxonomy} for {Fostering} {Information} {Literacy} in the {Era} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2503.19434},
	doi = {10.48550/ARXIV.2503.19434},
	abstract = {The advent of Large Language Models (LLMs) has profoundly transformed the paradigms of information retrieval and problem-solving, enabling students to access information acquisition more efficiently to support learning. However, there is currently a lack of standardized evaluation frameworks that guide learners in effectively leveraging LLMs. This paper proposes an LLM-driven Bloom's Educational Taxonomy that aims to recognize and evaluate students' information literacy (IL) with LLMs, and to formalize and guide students practice-based activities of using LLMs to solve complex problems. The framework delineates the IL corresponding to the cognitive abilities required to use LLM into two distinct stages: Exploration \&amp; Action and Creation \&amp; Metacognition. It further subdivides these into seven phases: Perceiving, Searching, Reasoning, Interacting, Evaluating, Organizing, and Curating. Through the case presentation, the analysis demonstrates the framework's applicability and feasibility, supporting its role in fostering IL among students with varying levels of prior knowledge. This framework fills the existing gap in the analysis of LLM usage frameworks and provides theoretical support for guiding learners to improve IL.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Luo, Yiming and Liu, Ting and Pang, Patrick Cheong-Iao and McKay, Dana and Chen, Ziqi and Buchanan, George and Chang, Shanton},
	year = {2025},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@misc{yaacoub_assessing_2025,
	title = {Assessing {AI}-{Generated} {Questions}' {Alignment} with {Cognitive} {Frameworks} in {Educational} {Assessment}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2504.14232},
	doi = {10.48550/ARXIV.2504.14232},
	abstract = {This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz, an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured framework for categorizing educational objectives into hierarchical cognitive levels. Our research investigates whether incorporating this taxonomy can improve the alignment of AI-generated questions with specific cognitive objectives. We developed a dataset of 3691 questions categorized according to Bloom's levels and employed various classification models-Multinomial Logistic Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a Transformer-based model (DistilBERT)-to evaluate their effectiveness in categorizing questions. Our results indicate that higher Bloom's levels generally correlate with increased question length, Flesch-Kincaid Grade Level (FKGL), and Lexical Density (LD), reflecting the increased complexity of higher cognitive demands. Multinomial Logistic Regression showed varying accuracy across Bloom's levels, performing best for "Knowledge" and less accurately for higher-order levels. Merging higher-level categories improved accuracy for complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective classification for lower levels but struggled with higher-order tasks. DistilBERT achieved the highest performance, significantly improving classification of both lower and higher-order cognitive levels, achieving an overall validation accuracy of 91\%. This study highlights the potential of integrating Bloom's Taxonomy into AI-driven assessment tools and underscores the advantages of advanced models like DistilBERT for enhancing educational content generation.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Yaacoub, Antoun and Da-Rugna, Jérôme and Assaghir, Zainab},
	year = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@article{elim_promoting_2024,
	title = {Promoting cognitive skills in {AI}-supported learning environments: the integration of bloom’s taxonomy},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {0300-4279, 1475-7575},
	shorttitle = {Promoting cognitive skills in {AI}-supported learning environments},
	url = {https://www.tandfonline.com/doi/full/10.1080/03004279.2024.2332469},
	doi = {10.1080/03004279.2024.2332469},
	language = {en},
	urldate = {2025-07-16},
	journal = {Education 3-13},
	author = {Elim, Emily Hui Sein Yue},
	month = apr,
	year = {2024},
	note = {Publisher: Informa UK Limited},
	pages = {1--11},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/3JIHPJEG/Elim - 2024 - Promoting cognitive skills in AI-supported learning environments the integration of bloom’s taxonom.pdf:application/pdf},
}

@article{hakkarainen_no_2002,
	title = {[{No} title found]},
	volume = {11},
	issn = {0926-7220},
	url = {http://link.springer.com/10.1023/A:1013076706416},
	doi = {10.1023/a:1013076706416},
	number = {1},
	urldate = {2025-07-16},
	journal = {Science and Education},
	author = {Hakkarainen, Kai and Sintonen, Matti},
	year = {2002},
	note = {Publisher: Springer Science and Business Media LLC},
	pages = {25--43},
}

@article{hakkarainen_interrogative_2002,
	title = {The {Interrogative} {Model} of {Inquiry} and {Computer}-{Supported} {Collaborative} {Learning}},
	volume = {11},
	issn = {0926-7220},
	url = {http://link.springer.com/10.1023/A:1013076706416},
	doi = {10.1023/a:1013076706416},
	number = {1},
	urldate = {2025-07-16},
	journal = {Science and Education},
	author = {Hakkarainen, Kai and Sintonen, Matti},
	year = {2002},
	note = {Publisher: Springer Science and Business Media LLC},
	pages = {25--43},
}

@book{koralus_reason_2023,
	address = {Oxford},
	title = {Reason and {Inquiry}: {The} {Erotetic} {Theory}},
	isbn = {978-0-19-255701-8},
	shorttitle = {Reason and {Inquiry}},
	abstract = {Reason and Inquiry: The Erotetic Theory presents a unified theory of the human capacity for reasoning and decision-making. The book's central idea is that our minds naturally aim at resolving issues, and if we are sufficiently inquisitive in the process, we can avoid mistakes},
	language = {eng},
	publisher = {Oxford University Press, Incorporated},
	author = {Koralus, Philipp},
	year = {2023},
}

@misc{koralus_pyetr_2025,
	title = {{PyETR}},
	copyright = {© 2025 Philipp Koralus, Sean Moss, Mark Todd},
	publisher = {University of Oxford; University of Birmingham; Dreaming Spires},
	author = {Koralus, Philipp and Moss, Sean and Todd, Mark},
	year = {2025},
}

@article{harrah_nuel_1978,
	title = {Nuel {D}. {BelnapJr}., and {Thomas} {B}. {Steel} {Jr}. {The} logic of questions and answers. {Yale} {University} {Press}, {New} {Haven} and {London1976}, vii + 209 pp. - {Urs} {Egli} and {Hubert} {Schleichert}. {Bibliography} of the theory of questions and answers. {Therein}, pp. 155–200.},
	volume = {43},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0022-4812, 1943-5886},
	url = {https://www.cambridge.org/core/product/identifier/S0022481200049744/type/journal_article},
	doi = {10.2307/2272838},
	language = {en},
	number = {2},
	urldate = {2025-07-16},
	journal = {Journal of Symbolic Logic},
	author = {Harrah, David},
	month = jun,
	year = {1978},
	note = {Publisher: Cambridge University Press (CUP)},
	pages = {379--380},
}

@book{belnap_logic_1976,
	address = {New Haven},
	title = {The logic of questions and answers},
	isbn = {978-0-300-01962-9},
	publisher = {Yale University Press},
	author = {Belnap, Nuel D. and Steel, Thomas B.},
	year = {1976},
	keywords = {Formal languages, Question (Logic), Question-answering systems},
}

@article{kirk_prism_2024,
	title = {The {PRISM} {Alignment} {Dataset}: {What} {Participatory}, {Representative} and {Individualised} {Human} {Feedback} {Reveals} {About} the {Subjective} and {Multicultural} {Alignment} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {The {PRISM} {Alignment} {Dataset}},
	url = {https://arxiv.org/abs/2404.16019},
	doi = {10.48550/ARXIV.2404.16019},
	abstract = {Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.},
	urldate = {2025-07-16},
	author = {Kirk, Hannah Rose and Whitefield, Alexander and Röttger, Paul and Bean, Andrew and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@misc{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BERT}},
	url = {https://arxiv.org/abs/1810.04805},
	doi = {10.48550/ARXIV.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@article{tran_investigation_2020,
	title = {An investigation into the flouting of conversational maxims employed by male and female guests in the {American} talk show "{The} {Ellen} {Show}"},
	issn = {1859-1531},
	url = {https://jst-ud.vn/jst-ud/article/view/3257},
	doi = {10.31130/jst-ud2020-069e},
	abstract = {Grice’s maxims are basic rules for interlocutors to follow in conversations. Nevertheless, when the speaker intentionally makes the hearer look for the real meaning beyond what is said implicitly, (s)he employs conversational maxim flouting. This article aims at investigating types of conversational maxim flouting and rhetorical strategies employed by male and female guests in the American talk show “The Ellen Show” and discovering similarities and differences in terms of the flouting of conversational maxims between two genders. The study design is based on a combination of qualitative and quantitative approaches and the application of descriptive and contrastive methods. The samples including 72 maxim flouting situations for each gender were selected from the Interviews section on the official channel of The Ellen Show. The findings reveal that although both genders share some similarities concerning the pragmatic features of maxim flouting, each gender reflects its own tendency of language style in communication.},
	urldate = {2025-07-24},
	journal = {Journal of Science and Technology Issue on Information and Communications Technology},
	author = {Tran, Nguyen Thi Quynh Hoa, Tran Thi Huyen Trang},
	month = jun,
	year = {2020},
	note = {Publisher: The University of Danang},
	pages = {117--122},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/9T5KQFYA/Tran - 2020 - An investigation into the flouting of conversational maxims employed by male and female gu.pdf:application/pdf},
}

@article{panzeri_childrens_2021,
	title = {Children’s and adults’ sensitivity to {Gricean} maxims and to the maximize presupposition principle},
	volume = {12},
	issn = {1664-1078},
	doi = {10.3389/fpsyg.2021.624628},
	abstract = {Up to age 5, children are known to experience difficulties in the derivation of implicitly conveyed content, sticking to literally true, even if underinformative, interpretation of sentences. The computation of implicated meanings is connected to the (apparent or manifest) violation of Gricean conversational maxims. We present a study that tests unmotivated violations of the maxims of Quantity, Relevance, and Manner and of the Maximize Presupposition principle, with a Truth Value Judgment task with three options of response. We tested pre-schoolers and school-aged children, with adults as controls, to verify at which age these pragmatic rules are recognized and to see whether there is a difference among these tenets. We found an evolutionary trend and that, in all age groups, violations of the maxims of Quantity and of Relation are sanctioned to a higher degree compared to infringements of the Maim of Manner and of the Maximize Presupposition principle. We conjecture that this relates to the effects that the violation of a certain maxim or principle has on the goals of the exchange: listeners are less tolerant with statements that transmit inaccurate or incomplete information, while being more tolerant with those that still permit to understand what has happened. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	journal = {Frontiers in Psychology},
	author = {Panzeri, Francesca and Foppolo, Francesca},
	year = {2021},
	note = {Place: Switzerland
Publisher: Frontiers Media S.A.},
	keywords = {Age Differences, Audiences, Conversation, Judgment, Measurement, Pragmatics, Preschool Students, Primary School Students, Tolerance, Trends, Truth},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/VCLKAWR3/Panzeri and Foppolo - 2021 - Children’s and adults’ sensitivity to Gricean maxims and to the maximize presupposition principle.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/VL8L5KUX/2021-28482-001.html:text/html},
}

@inproceedings{rottger_two_2022-1,
	address = {Seattle, United States},
	title = {Two {Contrasting} {Data} {Annotation} {Paradigms} for {Subjective} {NLP} {Tasks}},
	url = {https://aclanthology.org/2022.naacl-main.13},
	doi = {10.18653/v1/2022.naacl-main.13},
	abstract = {Labelled data is the foundation of most natural language processing tasks. However, labelling data is difﬁcult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss beneﬁts and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.},
	language = {en},
	urldate = {2025-07-16},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Röttger, Paul and Vidgen, Bertie and Hovy, Dirk and Pierrehumbert, Janet},
	year = {2022},
	file = {PDF:/Users/carolinewagner/Zotero/storage/ZQ2ZU3JB/Rottger et al. - 2022 - Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks.pdf:application/pdf},
}

@misc{laux_improving_2023-1,
	title = {Improving {Task} {Instructions} for {Data} {Annotators}: {How} {Clear} {Rules} and {Higher} {Pay} {Increase} {Performance} in {Data} {Annotation} in the {AI} {Economy}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Improving {Task} {Instructions} for {Data} {Annotators}},
	url = {https://arxiv.org/abs/2312.14565},
	abstract = {The global surge in AI applications is transforming industries, leading to displacement and complementation of existing jobs, while also giving rise to new employment opportunities. Data annotation, encompassing the labelling of images or annotating of texts by human workers, crucially influences the quality of a dataset directly influences the quality of AI models trained on it. This paper delves into the economics of data annotation, with a specific focus on the impact of task instruction design (that is, the choice between rules and standards as theorised in law and economics) and monetary incentives on data quality and costs. An experimental study involving 307 data annotators examines six groups with varying task instructions (norms) and monetary incentives. Results reveal that annotators provided with clear rules exhibit higher accuracy rates, outperforming those with vague standards by 14\%. Similarly, annotators receiving an additional monetary incentive perform significantly better, with the highest accuracy rate recorded in the group working with both clear rules and incentives (87.5\% accuracy). In addition, our results show that rules are perceived as being more helpful by annotators than standards and reduce annotators' difficulty in annotating images. These empirical findings underscore the double benefit of rule-based instructions on both data quality and worker wellbeing. Our research design allows us to reveal that, in our study, rules are more cost-efficient in increasing accuracy than monetary incentives. The paper contributes experimental insights to discussions on the economical, ethical, and legal considerations of AI technologies. Addressing policymakers and practitioners, we emphasise the need for a balanced approach in optimising data annotation processes for efficient and ethical AI development and usage.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Laux, Johann and Stephany, Fabian and Liefgreen, Alice},
	year = {2023},
	doi = {10.48550/ARXIV.2312.14565},
	keywords = {Applications (stat.AP), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Economics and business, General Economics (econ.GN)},
}

@book{grice_studies_1991-1,
	address = {Cambridge (Mass.) London},
	title = {Studies in the way of words},
	isbn = {978-0-674-85271-6},
	language = {eng},
	publisher = {Harvard university press},
	author = {Grice, H. Paul},
	year = {1991},
}

@article{kasirzadeh_conversation_2023-1,
	title = {In {Conversation} with {Artificial} {Intelligence}: {Aligning} language {Models} with {Human} {Values}},
	volume = {36},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2210-5433, 2210-5441},
	shorttitle = {In {Conversation} with {Artificial} {Intelligence}},
	url = {https://link.springer.com/10.1007/s13347-023-00606-x},
	doi = {10.1007/s13347-023-00606-x},
	abstract = {AbstractLarge-scale language technologies are increasingly used in various forms of communication with humans across different contexts. One particular use case for these technologies is conversational agents, which output natural language text in response to prompts and queries. This mode of engagement raises a number of social and ethical questions. For example, what does it mean to align conversational agents with human norms or values? Which norms or values should they be aligned with? And how can this be accomplished? In this paper, we propose a number of steps that help answer these questions. We start by developing a philosophical analysis of the building blocks of linguistic communication between conversational agents and human interlocutors. We then use this analysis to identify and formulate ideal norms of conversation that can govern successful linguistic communication between humans and conversational agents. Furthermore, we explore how these norms can be used to align conversational agents with human values across a range of different discursive domains. We conclude by discussing the practical implications of our proposal for the design of conversational agents that are aligned with these norms and values.},
	language = {en},
	number = {2},
	urldate = {2025-07-16},
	journal = {Philosophy \& Technology},
	author = {Kasirzadeh, Atoosa and Gabriel, Iason},
	month = jun,
	year = {2023},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/YNGBWTGP/Kasirzadeh and Gabriel - 2023 - In Conversation with Artificial Intelligence Aligning language Models with Human Values.pdf:application/pdf},
}

@misc{kim_applying_2025-1,
	title = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}: {Design} {Insights} from a {Participatory} {Approach}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}},
	url = {https://arxiv.org/abs/2503.00858},
	abstract = {While large language models (LLMs) are increasingly used to assist users in various tasks through natural language interactions, these interactions often fall short due to LLMs' limited ability to infer contextual nuances and user intentions, unlike humans. To address this challenge, we draw inspiration from the Gricean Maxims–human communication theory that suggests principles of effective communication–and aim to derive design insights for enhancing human-AI interactions (HAI). Through participatory design workshops with communication experts, designers, and end-users, we identified ways to apply these maxims across the stages of the HAI cycle. Our findings include reinterpreted maxims tailored to human-LLM contexts and nine actionable design considerations categorized by interaction stage. These insights provide a concrete framework for designing more cooperative and user-centered LLM-based systems, bridging theoretical foundations in communication with practical applications in HAI.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Kim, Yoonsu and Chin, Brandon and Son, Kihoon and Kim, Seoyoung and Kim, Juho},
	year = {2025},
	doi = {10.48550/ARXIV.2503.00858},
	keywords = {FOS: Computer and information sciences, Human-Computer Interaction (cs.HC)},
}

@misc{saad_gricean_2025-1,
	title = {Gricean {Norms} as a {Basis} for {Effective} {Collaboration}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2503.14484},
	abstract = {Effective human-AI collaboration hinges not only on the AI agent's ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks – common ground, relevance theory, and theory of mind – into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent's pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Saad, Fardin and Murukannaiah, Pradeep K. and Singh, Munindar P.},
	year = {2025},
	doi = {10.48550/ARXIV.2503.14484},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Computation and Language (cs.CL), Multiagent Systems (cs.MA)},
}

@misc{luo_enhanced_2025-1,
	title = {Enhanced {Bloom}'s {Educational} {Taxonomy} for {Fostering} {Information} {Literacy} in the {Era} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2503.19434},
	abstract = {The advent of Large Language Models (LLMs) has profoundly transformed the paradigms of information retrieval and problem-solving, enabling students to access information acquisition more efficiently to support learning. However, there is currently a lack of standardized evaluation frameworks that guide learners in effectively leveraging LLMs. This paper proposes an LLM-driven Bloom's Educational Taxonomy that aims to recognize and evaluate students' information literacy (IL) with LLMs, and to formalize and guide students practice-based activities of using LLMs to solve complex problems. The framework delineates the IL corresponding to the cognitive abilities required to use LLM into two distinct stages: Exploration \&amp; Action and Creation \&amp; Metacognition. It further subdivides these into seven phases: Perceiving, Searching, Reasoning, Interacting, Evaluating, Organizing, and Curating. Through the case presentation, the analysis demonstrates the framework's applicability and feasibility, supporting its role in fostering IL among students with varying levels of prior knowledge. This framework fills the existing gap in the analysis of LLM usage frameworks and provides theoretical support for guiding learners to improve IL.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Luo, Yiming and Liu, Ting and Pang, Patrick Cheong-Iao and McKay, Dana and Chen, Ziqi and Buchanan, George and Chang, Shanton},
	year = {2025},
	doi = {10.48550/ARXIV.2503.19434},
	keywords = {FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@misc{yaacoub_assessing_2025-1,
	title = {Assessing {AI}-{Generated} {Questions}' {Alignment} with {Cognitive} {Frameworks} in {Educational} {Assessment}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2504.14232},
	abstract = {This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz, an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured framework for categorizing educational objectives into hierarchical cognitive levels. Our research investigates whether incorporating this taxonomy can improve the alignment of AI-generated questions with specific cognitive objectives. We developed a dataset of 3691 questions categorized according to Bloom's levels and employed various classification models-Multinomial Logistic Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a Transformer-based model (DistilBERT)-to evaluate their effectiveness in categorizing questions. Our results indicate that higher Bloom's levels generally correlate with increased question length, Flesch-Kincaid Grade Level (FKGL), and Lexical Density (LD), reflecting the increased complexity of higher cognitive demands. Multinomial Logistic Regression showed varying accuracy across Bloom's levels, performing best for "Knowledge" and less accurately for higher-order levels. Merging higher-level categories improved accuracy for complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective classification for lower levels but struggled with higher-order tasks. DistilBERT achieved the highest performance, significantly improving classification of both lower and higher-order cognitive levels, achieving an overall validation accuracy of 91\%. This study highlights the potential of integrating Bloom's Taxonomy into AI-driven assessment tools and underscores the advantages of advanced models like DistilBERT for enhancing educational content generation.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Yaacoub, Antoun and Da-Rugna, Jérôme and Assaghir, Zainab},
	year = {2025},
	doi = {10.48550/ARXIV.2504.14232},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@article{elim_promoting_2024-1,
	title = {Promoting cognitive skills in {AI}-supported learning environments: the integration of bloom’s taxonomy},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {0300-4279, 1475-7575},
	shorttitle = {Promoting cognitive skills in {AI}-supported learning environments},
	url = {https://www.tandfonline.com/doi/full/10.1080/03004279.2024.2332469},
	doi = {10.1080/03004279.2024.2332469},
	language = {en},
	urldate = {2025-07-16},
	journal = {Education 3-13},
	author = {Elim, Emily Hui Sein Yue},
	month = apr,
	year = {2024},
	pages = {1--11},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/KQFW24D7/Elim - 2024 - Promoting cognitive skills in AI-supported learning environments the integration of bloom’s taxonom.pdf:application/pdf},
}

@article{hakkarainen_interrogative_2002-1,
	title = {The {Interrogative} {Model} of {Inquiry} and {Computer}-{Supported} {Collaborative} {Learning}},
	volume = {11},
	issn = {0926-7220},
	url = {http://link.springer.com/10.1023/A:1013076706416},
	doi = {10.1023/a:1013076706416},
	number = {1},
	urldate = {2025-07-16},
	journal = {Science and Education},
	author = {Hakkarainen, Kai and Sintonen, Matti},
	year = {2002},
	pages = {25--43},
}

@book{koralus_reason_2023-1,
	address = {Oxford},
	title = {Reason and {Inquiry}: {The} {Erotetic} {Theory}},
	isbn = {978-0-19-255701-8},
	shorttitle = {Reason and {Inquiry}},
	abstract = {Reason and Inquiry: The Erotetic Theory presents a unified theory of the human capacity for reasoning and decision-making. The book's central idea is that our minds naturally aim at resolving issues, and if we are sufficiently inquisitive in the process, we can avoid mistakes},
	language = {eng},
	publisher = {Oxford University Press, Incorporated},
	author = {Koralus, Philipp},
	year = {2023},
}

@misc{koralus_pyetr_2025-1,
	title = {{PyETR}},
	copyright = {© 2025 Philipp Koralus, Sean Moss, Mark Todd},
	publisher = {University of Oxford; University of Birmingham; Dreaming Spires},
	author = {Koralus, Philipp and Moss, Sean and Todd, Mark},
	year = {2025},
}

@article{kirk_prism_2024-1,
	title = {The {PRISM} {Alignment} {Dataset}: {What} {Participatory}, {Representative} and {Individualised} {Human} {Feedback} {Reveals} {About} the {Subjective} and {Multicultural} {Alignment} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {The {PRISM} {Alignment} {Dataset}},
	url = {https://arxiv.org/abs/2404.16019},
	doi = {10.48550/ARXIV.2404.16019},
	abstract = {Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.},
	urldate = {2025-07-16},
	author = {Kirk, Hannah Rose and Whitefield, Alexander and Röttger, Paul and Bean, Andrew and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
	year = {2024},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@misc{egami_using_2023,
	title = {Using {Imperfect} {Surrogates} for {Downstream} {Inference}: {Design}-based {Supervised} {Learning} for {Social} {Science} {Applications} of {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Using {Imperfect} {Surrogates} for {Downstream} {Inference}},
	url = {https://arxiv.org/abs/2306.04746},
	abstract = {In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. One increasingly common way to annotate documents cheaply at scale is through large language models (LLMs). However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased. We present a new algorithm for using imperfect annotation surrogates for downstream statistical analyses while guaranteeing statistical properties – like asymptotic unbiasedness and proper uncertainty quantification – which are fundamental to CSS research. We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80-90\%. To address this, we build on debiased machine learning to propose the design-based supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without inferential guarantees.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Egami, Naoki and Hinck, Musashi and Stewart, Brandon M. and Wei, Hanying},
	year = {2023},
	doi = {10.48550/ARXIV.2306.04746},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), Methodology (stat.ME)},
}

@misc{devlin_bert_2018-1,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BERT}},
	url = {https://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@misc{brachman_current_2025,
	title = {Current and {Future} {Use} of {Large} {Language} {Models} for {Knowledge} {Work}},
	url = {http://arxiv.org/abs/2503.16774},
	doi = {10.48550/arXiv.2503.16774},
	abstract = {Large Language Models (LLMs) have introduced a paradigm shift in interaction with AI technology, enabling knowledge workers to complete tasks by specifying their desired outcome in natural language. LLMs have the potential to increase productivity and reduce tedious tasks in an unprecedented way. A systematic study of LLM adoption for work can provide insight into how LLMs can best support these workers. To explore knowledge workers' current and desired usage of LLMs, we ran a survey (n=216). Workers described tasks they already used LLMs for, like generating code or improving text, but imagined a future with LLMs integrated into their workflows and data. We ran a second survey (n=107) a year later that validated our initial findings and provides insight into up-to-date LLM use by knowledge workers. We discuss implications for adoption and design of generative AI technologies for knowledge work.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Brachman, Michelle and El-Ashry, Amina and Dugan, Casey and Geyer, Werner},
	month = mar,
	year = {2025},
	note = {arXiv:2503.16774 [cs]
version: 1},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/3CZ9HWZ7/Brachman et al. - 2025 - Current and Future Use of Large Language Models for Knowledge Work.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/FKSN3VS4/2503.html:text/html},
}

@misc{chen_spiral_2024,
	title = {Spiral of {Silence}: {How} is {Large} {Language} {Model} {Killing} {Information} {Retrieval}? -- {A} {Case} {Study} on {Open} {Domain} {Question} {Answering}},
	shorttitle = {Spiral of {Silence}},
	url = {http://arxiv.org/abs/2404.10496},
	doi = {10.48550/arXiv.2404.10496},
	abstract = {The practice of Retrieval-Augmented Generation (RAG), which integrates Large Language Models (LLMs) with retrieval systems, has become increasingly prevalent. However, the repercussions of LLM-derived content infiltrating the web and influencing the retrieval-generation feedback loop are largely uncharted territories. In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems. Taking the trending Open Domain Question Answering (ODQA) task as a point of entry, our findings reveal a potential digital "Spiral of Silence" effect, with LLM-generated text consistently outperforming human-authored content in search rankings, thereby diminishing the presence and impact of human contributions online. This trend risks creating an imbalanced information ecosystem, where the unchecked proliferation of erroneous LLM-generated content may result in the marginalization of accurate information. We urge the academic community to take heed of this potential issue, ensuring a diverse and authentic digital information landscape.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Chen, Xiaoyang and He, Ben and Lin, Hongyu and Han, Xianpei and Wang, Tianshu and Cao, Boxi and Sun, Le and Sun, Yingfei},
	month = jun,
	year = {2024},
	note = {arXiv:2404.10496 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/QWKG8HKE/Chen et al. - 2024 - Spiral of Silence How is Large Language Model Killing Information Retrieval -- A Case Study on Ope.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/4J8GXLUZ/2404.html:text/html},
}

@misc{weidinger_ethical_2021,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	doi = {10.48550/arXiv.2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/IZS7LZ2R/Weidinger et al. - 2021 - Ethical and social risks of harm from Language Models.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/AIAQUS89/2112.html:text/html},
}

@misc{tamkin_understanding_2021,
	title = {Understanding the {Capabilities}, {Limitations}, and {Societal} {Impact} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2102.02503},
	doi = {10.48550/arXiv.2102.02503},
	abstract = {On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Tamkin, Alex and Brundage, Miles and Clark, Jack and Ganguli, Deep},
	month = feb,
	year = {2021},
	note = {arXiv:2102.02503 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/49JYMTBV/Tamkin et al. - 2021 - Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/N24CKYYE/2102.html:text/html},
}

@incollection{koralus_decision_2022,
	edition = {1},
	title = {Decision and {Practical} {Reasoning}},
	isbn = {978-0-19-882376-6 978-0-19-186254-0},
	url = {https://academic.oup.com/book/45443/chapter/389466704},
	abstract = {Abstract
            This chapter extends the erotetic theory to decision-making, allowing the definition of a notion of an erotetic agent. The chapter diagnoses widely discussed decision-making phenomena like framing effects and the endowment effect as structurally similar to the reasoning fallacies discussed in previous chapters. The chapter obtains that, as in previous chapters, erotetic equilibrium secures classical standards of rationality, in this case classically rational choice. The chapter also considers how erotetic reasons-based decision-making can facilitate rapid action, making sense of what are sometimes termed “affordances.” Finally, the chapter considers unique threats the erotetic agent faces, and proposes an argument that deliberative democracy has particular legitimacy for erotetic agents: collective judgment at the end of a suitable deliberative process is a better reflection of group agency because it tends to be a better reflection of individual agency.},
	language = {en},
	urldate = {2025-07-30},
	booktitle = {Reason and {Inquiry}},
	publisher = {Oxford University PressOxford},
	author = {Koralus, Philipp and Moss, Sean},
	collaborator = {Koralus, Philipp},
	month = dec,
	year = {2022},
	doi = {10.1093/oso/9780198823766.003.0006},
	pages = {248--301},
	file = {PDF:/Users/carolinewagner/Zotero/storage/KRGK89XD/Koralus - 2022 - Decision and Practical Reasoning.pdf:application/pdf},
}

@misc{sclar_quantifying_2024,
	title = {Quantifying {Language} {Models}' {Sensitivity} to {Spurious} {Features} in {Prompt} {Design} or: {How} {I} learned to start worrying about prompt formatting},
	shorttitle = {Quantifying {Language} {Models}' {Sensitivity} to {Spurious} {Features} in {Prompt} {Design} or},
	url = {http://arxiv.org/abs/2310.11324},
	doi = {10.48550/arXiv.2310.11324},
	abstract = {As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.},
	urldate = {2025-07-30},
	publisher = {arXiv},
	author = {Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
	month = jul,
	year = {2024},
	note = {arXiv:2310.11324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/UHF5BMF3/Sclar et al. - 2024 - Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or How I learned to.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/LVWABSXP/2310.html:text/html},
}

@misc{razavi_benchmarking_2025,
	title = {Benchmarking {Prompt} {Sensitivity} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2502.06065},
	doi = {10.48550/arXiv.2502.06065},
	abstract = {Large language Models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset PromptSET designed to investigate the effects of slight prompt variations on LLM performance. Using TriviaQA and HotpotQA datasets as the foundation of our work, we generate prompt variations and evaluate their effectiveness across multiple LLMs. We benchmark the prompt sensitivity prediction task employing state-of-the-art methods from related tasks, including LLM-based self-evaluation, text classification, and query performance prediction techniques. Our findings reveal that existing methods struggle to effectively address prompt sensitivity prediction, underscoring the need to understand how information needs should be phrased for accurate LLM responses.},
	urldate = {2025-07-30},
	publisher = {arXiv},
	author = {Razavi, Amirhossein and Soltangheis, Mina and Arabzadeh, Negar and Salamat, Sara and Zihayat, Morteza and Bagheri, Ebrahim},
	month = feb,
	year = {2025},
	note = {arXiv:2502.06065 [cs]
version: 1},
	keywords = {Computer Science - Information Retrieval, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/W88G4CFC/Razavi et al. - 2025 - Benchmarking Prompt Sensitivity in Large Language Models.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/TWZQHMXF/2502.html:text/html},
}

@article{jiang_how_2020,
	title = {How {Can} {We} {Know} {What} {Language} {Models} {Know}?},
	volume = {8},
	url = {https://aclanthology.org/2020.tacl-1.28/},
	doi = {10.1162/tacl_a_00324},
	abstract = {Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a \_\_ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a \_\_ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.},
	urldate = {2025-08-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
	editor = {Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2020},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {423--438},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/H8QK775A/Jiang et al. - 2020 - How Can We Know What Language Models Know.pdf:application/pdf},
}

@inproceedings{press_measuring_2023,
	address = {Singapore},
	title = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.378/},
	doi = {10.18653/v1/2023.findings-emnlp.378},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	urldate = {2025-08-01},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah and Lewis, Mike},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {5687--5711},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/TDQXI65Y/Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap in Language Models.pdf:application/pdf},
}

@misc{li_large_2023,
	title = {Large {Language} {Models} {Understand} and {Can} be {Enhanced} by {Emotional} {Stimuli}},
	url = {http://arxiv.org/abs/2307.11760},
	doi = {10.48550/arXiv.2307.11760},
	abstract = {Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call "EmotionPrompt" that combines the original prompt with emotional stimuli), e.g., 8.00\% relative performance improvement in Instruction Induction and 115\% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9\% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.},
	urldate = {2025-08-01},
	publisher = {arXiv},
	author = {Li, Cheng and Wang, Jindong and Zhang, Yixuan and Zhu, Kaijie and Hou, Wenxin and Lian, Jianxun and Luo, Fang and Yang, Qiang and Xie, Xing},
	month = nov,
	year = {2023},
	note = {arXiv:2307.11760 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/AVPQHBJS/Li et al. - 2023 - Large Language Models Understand and Can be Enhanced by Emotional Stimuli.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/LITTPD5H/2307.html:text/html},
}

@article{gaber_evaluating_2025,
	title = {Evaluating large language model workflows in clinical decision support for triage and referral and diagnosis},
	volume = {8},
	copyright = {2025 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-025-01684-1},
	doi = {10.1038/s41746-025-01684-1},
	abstract = {Accurate medical decision-making is critical for both patients and clinicians. Patients often struggle to interpret their symptoms, determine their severity, and select the right specialist. Simultaneously, clinicians face challenges in integrating complex patient data to make timely, accurate diagnoses. Recent advances in large language models (LLMs) offer the potential to bridge this gap by supporting decision-making for both patients and healthcare providers. In this study, we benchmark multiple LLM versions and an LLM-based workflow incorporating retrieval-augmented generation (RAG) on a curated dataset of 2000 medical cases derived from the Medical Information Mart for Intensive Care database. Our findings show that these LLMs are capable of providing personalized insights into likely diagnoses, suggesting appropriate specialists, and assessing urgent care needs. These models may also support clinicians in refining diagnoses and decision-making, offering a promising approach to improving patient outcomes and streamlining healthcare delivery.},
	language = {en},
	number = {1},
	urldate = {2025-08-01},
	journal = {npj Digital Medicine},
	author = {Gaber, Farieda and Shaik, Maqsood and Allega, Fabio and Bilecz, Agnes Julia and Busch, Felix and Goon, Kelsey and Franke, Vedran and Akalin, Altuna},
	month = may,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Diseases, Medical research, Signs and symptoms},
	pages = {263},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/6VL2AM4P/Gaber et al. - 2025 - Evaluating large language model workflows in clinical decision support for triage and referral and d.pdf:application/pdf},
}

@misc{ovadya_bridging_2023,
	title = {Bridging {Systems}: {Open} {Problems} for {Countering} {Destructive} {Divisiveness} across {Ranking}, {Recommenders}, and {Governance}},
	shorttitle = {Bridging {Systems}},
	url = {http://arxiv.org/abs/2301.09976},
	doi = {10.48550/arXiv.2301.09976},
	abstract = {Divisiveness appears to be increasing in much of the world, leading to concern about political violence and a decreasing capacity to collaboratively address large-scale societal challenges. In this working paper we aim to articulate an interdisciplinary research and practice area focused on what we call bridging systems: systems which increase mutual understanding and trust across divides, creating space for productive conflict, deliberation, or cooperation. We give examples of bridging systems across three domains: recommender systems on social media, collective response systems, and human-facilitated group deliberation. We argue that these examples can be more meaningfully understood as processes for attention-allocation (as opposed to "content distribution" or "amplification") and develop a corresponding framework to explore similarities - and opportunities for bridging - across these seemingly disparate domains. We focus particularly on the potential of bridging-based ranking to bring the benefits of offline bridging into spaces which are already governed by algorithms. Throughout, we suggest research directions that could improve our capacity to incorporate bridging into a world increasingly mediated by algorithms and artificial intelligence.},
	urldate = {2025-08-03},
	publisher = {arXiv},
	author = {Ovadya, Aviv and Thorburn, Luke},
	month = jul,
	year = {2023},
	note = {arXiv:2301.09976 [cs]},
	keywords = {Computer Science - Social and Information Networks},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/YZQ2GRLP/Ovadya and Thorburn - 2023 - Bridging Systems Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/2ZXGK8FU/2301.html:text/html},
}

@misc{pearl_understanding_2013,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Understanding {Simpson}'s {Paradox}},
	url = {https://papers.ssrn.com/abstract=2343788},
	doi = {10.2139/ssrn.2343788},
	abstract = {Simpson's paradox is often presented as a compelling demonstration of why we need statistics education in our schools. It is a reminder of how easy it is to fall into a web of paradoxical conclusions when relying solely on intuition, unaided by rigorous statistical methods. In recent years, ironically, the paradox assumed an added dimension when educators began using it to demonstrate the limits of statistical methods, and why causal, rather than statistical considerations are necessary to avoid those paradoxical conclusions (Arah, 2008; Pearl, 2009, pp. 173-182; Wasserman, 2004).},
	language = {en},
	urldate = {2025-08-06},
	publisher = {Social Science Research Network},
	author = {Pearl, Judea},
	month = sep,
	year = {2013},
	keywords = {Judea Pearl, SSRN, Understanding Simpson's Paradox},
	file = {Submitted Version:/Users/carolinewagner/Zotero/storage/DQ9PWLPX/Pearl - 2013 - Understanding Simpson's Paradox.pdf:application/pdf},
}

@article{gaber_evaluating_2025-1,
	title = {Evaluating large language model workflows in clinical decision support for triage and referral and diagnosis},
	volume = {8},
	copyright = {2025 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-025-01684-1},
	doi = {10.1038/s41746-025-01684-1},
	abstract = {Accurate medical decision-making is critical for both patients and clinicians. Patients often struggle to interpret their symptoms, determine their severity, and select the right specialist. Simultaneously, clinicians face challenges in integrating complex patient data to make timely, accurate diagnoses. Recent advances in large language models (LLMs) offer the potential to bridge this gap by supporting decision-making for both patients and healthcare providers. In this study, we benchmark multiple LLM versions and an LLM-based workflow incorporating retrieval-augmented generation (RAG) on a curated dataset of 2000 medical cases derived from the Medical Information Mart for Intensive Care database. Our findings show that these LLMs are capable of providing personalized insights into likely diagnoses, suggesting appropriate specialists, and assessing urgent care needs. These models may also support clinicians in refining diagnoses and decision-making, offering a promising approach to improving patient outcomes and streamlining healthcare delivery.},
	language = {en},
	number = {1},
	urldate = {2025-08-10},
	journal = {npj Digital Medicine},
	author = {Gaber, Farieda and Shaik, Maqsood and Allega, Fabio and Bilecz, Agnes Julia and Busch, Felix and Goon, Kelsey and Franke, Vedran and Akalin, Altuna},
	month = may,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Diseases, Medical research, Signs and symptoms},
	pages = {263},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/KF58I7ZI/Gaber et al. - 2025 - Evaluating large language model workflows in clinical decision support for triage and referral and d.pdf:application/pdf},
}

@article{rydbeck_younger_2021,
	title = {Younger age at onset of colorectal cancer is associated with increased patient’s delay},
	volume = {154},
	issn = {0959-8049, 1879-0852},
	url = {https://www.ejcancer.com/article/S0959-8049(21)00395-6/fulltext},
	doi = {10.1016/j.ejca.2021.06.020},
	language = {English},
	urldate = {2025-08-10},
	journal = {European Journal of Cancer},
	author = {Rydbeck, Daniel and Asplund, Dan and Bock, David and Haglind, Eva and Park, Jennifer and Rosenberg, Jacob and Walming, Sofie and Angenete, Eva},
	month = sep,
	year = {2021},
	pmid = {34298377},
	note = {Publisher: Elsevier},
	keywords = {Age groups, Colorectal neoplasm, Delayed diagnosis, Symptoms},
	pages = {269--276},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/BFCQIZRQ/Rydbeck et al. - 2021 - Younger age at onset of colorectal cancer is associated with increased patient’s delay.pdf:application/pdf},
}

@article{caluwaerts_deliberation_2023,
	title = {Deliberation and polarization: a multi-disciplinary review},
	volume = {5},
	issn = {2673-3145},
	shorttitle = {Deliberation and polarization},
	url = {https://www.frontiersin.org/journals/political-science/articles/10.3389/fpos.2023.1127372/full},
	doi = {10.3389/fpos.2023.1127372},
	abstract = {In recent years, deliberative democracy has drawn attention as a potential way of fighting polarization. Allowing citizens to exchange arguments and viewpoints on political issues in group, can have strong conflict-mitigating effects: it can foster opinion changes (thereby overcoming idea-based polarization), and improve relations between diametrically opposed groups (thereby tackling affective forms of polarization, such as affective polarization). However, these results conflict with social psychological and communication studies which find that communicative encounters between groups can lead to further polarization and even group think. The question therefore arises under which conditions deliberative interactions between citizens can decrease polarization. Based on a multidisciplinary systematic review of the literature, which includes a wide diversity of communicative encounters ranging from short classroom discussions to multi-weekend citizen assemblies, this paper reports several findings. First, we argue that the effects of communicative encounters on polarization are conditional on how those types of communication were conceptualized across disciplines. More precisely, we find depolarizing effects when group discussions adhere to a deliberative democracy framework, and polarizing effects when they do not. Second we find that the depolarizing effects depend on several design factors that are often implemented in deliberative democracy studies. Finally, our analysis shows that that much more work needs to be done to unravel and test the exact causal mechanism(s) underlying the polarization-reducing effects of deliberation. Many potential causal mechanisms were identified, but few studies were able to adjudicate how deliberation affects polarization.},
	language = {English},
	urldate = {2025-08-10},
	journal = {Frontiers in Political Science},
	author = {Caluwaerts, Didier and Bernaerts, Kamil and Kesberg, Rebekka and Smets, Lien and Spruyt, Bram},
	month = jun,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {Affective polarization, deliberative democracy, Democracy, polarization, Systematic reveiw},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/5VZ9MDIS/Caluwaerts et al. - 2023 - Deliberation and polarization a multi-disciplinary review.pdf:application/pdf},
}

@misc{noauthor_generically_nodate,
	title = {Generically partisan: {Polarization} in political communication},
	shorttitle = {Generically partisan},
	url = {https://www.pnas.org/doi/epdf/10.1073/pnas.2309361120},
	language = {en},
	urldate = {2025-08-10},
	doi = {10.1073/pnas.2309361120},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/K6L73XPN/Generically partisan Polarization in political communication.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/SVRZCRDF/pnas.html:text/html},
}

@article{stewart_crowdsourcing_2017,
	title = {Crowdsourcing {Samples} in {Cognitive} {Science}},
	volume = {21},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661317301316},
	doi = {10.1016/j.tics.2017.06.007},
	abstract = {Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.},
	number = {10},
	urldate = {2025-08-10},
	journal = {Trends in Cognitive Sciences},
	author = {Stewart, Neil and Chandler, Jesse and Paolacci, Gabriele},
	month = oct,
	year = {2017},
	pages = {736--748},
	file = {ScienceDirect Full Text PDF:/Users/carolinewagner/Zotero/storage/IYLGQ82Z/Stewart et al. - 2017 - Crowdsourcing Samples in Cognitive Science.pdf:application/pdf;ScienceDirect Snapshot:/Users/carolinewagner/Zotero/storage/5U9CLBQX/S1364661317301316.html:text/html},
}

@article{kyrychenko_profiling_2025,
	title = {Profiling misinformation susceptibility},
	volume = {241},
	issn = {0191-8869},
	url = {https://www.sciencedirect.com/science/article/pii/S0191886925001394},
	doi = {10.1016/j.paid.2025.113177},
	abstract = {The global spread of misinformation poses a serious threat to the functioning of societies worldwide. But who falls for it? In this study, 66,242 individuals from 24 countries completed the Misinformation Susceptibility Test (MIST) and indicated their self-perceived misinformation discernment ability. Multilevel modelling showed that Generation Z, non-male, less educated, and more conservative individuals were more vulnerable to misinformation. Furthermore, while individuals' confidence in detecting misinformation was generally associated with better actual discernment, the degree to which perceived ability matched actual ability varied across subgroups. That is, whereas women were especially accurate in assessing their ability, extreme conservatives' perceived ability showed little relation to their actual misinformation discernment. Meanwhile, across all generations, Gen Z perceived their misinformation discernment ability most accurately, despite performing worst on the test. Taken together, our analyses provide the first systematic and holistic profile of misinformation susceptibility.},
	urldate = {2025-08-10},
	journal = {Personality and Individual Differences},
	author = {Kyrychenko, Yara and Koo, Hyunjin J. and Maertens, Rakoen and Roozenbeek, Jon and van der Linden, Sander and Götz, Friedrich M.},
	month = jul,
	year = {2025},
	keywords = {Conspiracy theories, Fake news, Misinformation susceptibility, MIST},
	pages = {113177},
}

@misc{noauthor_design-based_nodate,
	title = {Design-based {Supervised} {Learning}},
	url = {https://naokiegami.com/dsl/},
	abstract = {R package dsl implements design-based supervised learning (DSL) proposed in Egami, Hinck, Stewart, and Wei (2023). DSL is a general estimation framework for using predicted variables in statistical analyses. The package is especially useful for researchers trying to use large language models (LLMs) to annotate a large number of documents they analyze subsequently. dsl allows users to obtain statistically valid estimates and standard errors, even when LLM annotations contain arbitrary non-random prediction errors and biases.},
	language = {en},
	urldate = {2025-08-10},
	file = {Snapshot:/Users/carolinewagner/Zotero/storage/VYGPAFQ8/dsl.html:text/html},
}

@article{kasirzadeh_conversation_2023-2,
	title = {In {Conversation} with {Artificial} {Intelligence}: {Aligning} language {Models} with {Human} {Values}},
	volume = {36},
	issn = {2210-5441},
	shorttitle = {In {Conversation} with {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s13347-023-00606-x},
	doi = {10.1007/s13347-023-00606-x},
	abstract = {Large-scale language technologies are increasingly used in various forms of communication with humans across different contexts. One particular use case for these technologies is conversational agents, which output natural language text in response to prompts and queries. This mode of engagement raises a number of social and ethical questions. For example, what does it mean to align conversational agents with human norms or values? Which norms or values should they be aligned with? And how can this be accomplished? In this paper, we propose a number of steps that help answer these questions. We start by developing a philosophical analysis of the building blocks of linguistic communication between conversational agents and human interlocutors. We then use this analysis to identify and formulate ideal norms of conversation that can govern successful linguistic communication between humans and conversational agents. Furthermore, we explore how these norms can be used to align conversational agents with human values across a range of different discursive domains. We conclude by discussing the practical implications of our proposal for the design of conversational agents that are aligned with these norms and values.},
	language = {en},
	number = {2},
	urldate = {2025-08-11},
	journal = {Philosophy \& Technology},
	author = {Kasirzadeh, Atoosa and Gabriel, Iason},
	month = apr,
	year = {2023},
	keywords = {Artificial intelligence, Conversational agents, Ethics of language models, Language technologies, Large language models, Value alignment},
	pages = {27},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/JL6LPCZU/Kasirzadeh and Gabriel - 2023 - In Conversation with Artificial Intelligence Aligning language Models with Human Values.pdf:application/pdf},
}

@techreport{bick_rapid_2024,
	title = {The {Rapid} {Adoption} of {Generative} {AI}},
	url = {https://s3.amazonaws.com/real.stlouisfed.org/wp/2024/2024-027.pdf},
	abstract = {Generative artificial intelligence (AI) is a potentially important new technology, but its impact on the economy depends on the speed and intensity of adoption. This paper reports results from a series of nationally representative U.S. surveys of generative AI use at work and at home. As of late 2024, nearly 40\% of the U.S. population age 18-64 uses generative AI. Among employed respondents, 23\% used generative AI for work at least once in the previous week: 9\% used it every workday, and 14\% on some but not all workdays. Relative to each technology’s first mass-market product launch, work adoption of generative AI has been as fast as the personal computer (PC), and overall adoption has been faster than either PCs or the internet. Generative AI and PCs have very similar early work adoption patterns by education, occupation, and other characteristics. Between 1 and 5\% of all work hours are currently assisted by generative AI, and respondents report time savings equivalent to 1.4\% of total work hours. This suggests that substantial productivity gains from generative AI are possible.},
	language = {en},
	urldate = {2025-08-11},
	author = {Bick, Alexander and Blandin, Adam and Deming, David J.},
	year = {2024},
	doi = {10.20955/wp.2024.027},
	file = {PDF:/Users/carolinewagner/Zotero/storage/WEIH8E7K/Bick et al. - 2024 - The Rapid Adoption of Generative AI.pdf:application/pdf},
}

@misc{bick_rapid_2024-1,
	type = {Working {Paper}},
	series = {Working {Paper} {Series}},
	title = {The {Rapid} {Adoption} of {Generative} {AI}},
	url = {https://www.nber.org/papers/w32966},
	doi = {10.3386/w32966},
	abstract = {Generative artificial intelligence (AI) is a potentially important new technology, but its impact on the economy depends on the speed and intensity of adoption. This paper reports results from a series of nationally representative U.S. surveys of generative AI use at work and at home. As of late 2024, nearly 40 percent of the U.S. population age 18-64 uses generative AI. 23 percent of employed respondents had used generative AI for work at least once in the previous week, and 9 percent used it every work day. Relative to each technology’s first mass-market product launch, work adoption of generative AI has been as fast as the personal computer (PC), and overall adoption has been faster than either PCs or the internet. Generative AI and PCs have very similar early adoption patterns by education, occupation, and other characteristics. Between 1 and 5 percent of all work hours are currently assisted by generative AI, and respondents report time savings equivalent to 1.4 percent of total work hours. This suggests that substantial productivity gains from generative AI are possible.},
	urldate = {2025-08-11},
	publisher = {National Bureau of Economic Research},
	author = {Bick, Alexander and Blandin, Adam and Deming, David J.},
	month = sep,
	year = {2024},
	doi = {10.3386/w32966},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/LFDX5VNI/Bick et al. - 2024 - The Rapid Adoption of Generative AI.pdf:application/pdf},
}

@article{thapa_large_2025,
	title = {Large language models ({LLM}) in computational social science: prospects, current state, and challenges},
	volume = {15},
	issn = {1869-5469},
	shorttitle = {Large language models ({LLM}) in computational social science},
	url = {https://doi.org/10.1007/s13278-025-01428-9},
	doi = {10.1007/s13278-025-01428-9},
	abstract = {The advent of large language models (LLMs) has marked a new era in the transformation of computational social science (CSS). This paper dives into the role of LLMs in CSS, particularly exploring their potential to revolutionize data analysis and content generation and contribute to a broader understanding of social phenomena. We begin by discussing the applications of LLMs in various computational problems in social science including sentiment analysis, hate speech detection, stance and humor detection, misinformation detection, event understanding, and social network analysis, illustrating their capacity to generate nuanced insights into human behavior and societal trends. Furthermore, we explore the innovative use of LLMs in generating social media content. We also discuss the various ethical, technical, and legal issues these applications pose, and considerations required for responsible LLM usage. We further present the challenges associated with data bias, privacy, and the integration of these models into existing research frameworks. This paper aims to provide a solid background on the potential of LLMs in CSS, their past applications, current problems, and how they can pave the way for revolutionizing CSS.},
	language = {en},
	number = {1},
	urldate = {2025-08-11},
	journal = {Social Network Analysis and Mining},
	author = {Thapa, Surendrabikram and Shiwakoti, Shuvam and Shah, Siddhant Bikram and Adhikari, Surabhi and Veeramani, Hariram and Nasim, Mehwish and Naseem, Usman},
	month = mar,
	year = {2025},
	keywords = {Large language models, Computational social science, Natural language processing, Social network analysis, Social science},
	pages = {4},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/KJIDS44D/Thapa et al. - 2025 - Large language models (LLM) in computational social science prospects, current state, and challenge.pdf:application/pdf},
}

@misc{noauthor_get_nodate,
	title = {Get {Started} with dsl},
	url = {https://naokiegami.com/dsl/articles/intro.html},
	language = {en},
	urldate = {2025-08-12},
	file = {Snapshot:/Users/carolinewagner/Zotero/storage/YRRLGUZY/intro.html:text/html},
}

@misc{pavlovic_understanding_2025,
	title = {Understanding {Model} {Calibration} -- {A} gentle introduction and visual exploration of calibration and the expected calibration error ({ECE})},
	url = {http://arxiv.org/abs/2501.19047},
	doi = {10.48550/arXiv.2501.19047},
	abstract = {To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.},
	urldate = {2025-08-12},
	publisher = {arXiv},
	author = {Pavlovic, Maja},
	month = may,
	year = {2025},
	note = {arXiv:2501.19047 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Statistics - Methodology},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/ZJ9IAM4N/Pavlovic - 2025 - Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and t.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/SXQSHLRM/2501.html:text/html},
}

@misc{guo_calibration_2017,
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.04599},
	doi = {10.48550/arXiv.1706.04599},
	abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
	urldate = {2025-08-12},
	publisher = {arXiv},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	month = aug,
	year = {2017},
	note = {arXiv:1706.04599 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/3U8KGGQQ/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/3SZCINH2/1706.html:text/html},
}

@misc{lees_new_2022,
	title = {A {New} {Generation} of {Perspective} {API}: {Efficient} {Multilingual} {Character}-level {Transformers}},
	shorttitle = {A {New} {Generation} of {Perspective} {API}},
	url = {http://arxiv.org/abs/2202.11176},
	doi = {10.48550/arXiv.2202.11176},
	abstract = {On the world wide web, toxic content detectors are a crucial line of defense against potentially hateful and offensive messages. As such, building highly effective classifiers that enable a safer internet is an important research area. Moreover, the web is a highly multilingual, cross-cultural community that develops its own lingo over time. As such, it is crucial to develop models that are effective across a diverse range of languages, usages, and styles. In this paper, we present the fundamentals behind the next version of the Perspective API from Google Jigsaw. At the heart of the approach is a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks. We demonstrate that by forgoing static vocabularies, we gain flexibility across a variety of settings. We additionally outline the techniques employed to make such a byte-level model efficient and feasible for productionization. Through extensive experiments on multilingual toxic comment classification benchmarks derived from real API traffic and evaluation on an array of code-switching, covert toxicity, emoji-based hate, human-readable obfuscation, distribution shift, and bias evaluation settings, we show that our proposed approach outperforms strong baselines. Finally, we present our findings from deploying this system in production.},
	urldate = {2025-08-12},
	publisher = {arXiv},
	author = {Lees, Alyssa and Tran, Vinh Q. and Tay, Yi and Sorensen, Jeffrey and Gupta, Jai and Metzler, Donald and Vasserman, Lucy},
	month = feb,
	year = {2022},
	note = {arXiv:2202.11176 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/55DRKB9K/Lees et al. - 2022 - A New Generation of Perspective API Efficient Multilingual Character-level Transformers.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/QXZ9RWC7/2202.html:text/html},
}

@misc{schulhoff_prompt_2025,
	title = {The {Prompt} {Report}: {A} {Systematic} {Survey} of {Prompt} {Engineering} {Techniques}},
	shorttitle = {The {Prompt} {Report}},
	url = {http://arxiv.org/abs/2406.06608},
	doi = {10.48550/arXiv.2406.06608},
	abstract = {Generative Artificial Intelligence (GenAI) systems are increasingly being deployed across diverse industries and research domains. Developers and end-users interact with these systems through the use of prompting and prompt engineering. Although prompt engineering is a widely adopted and extensively researched area, it suffers from conflicting terminology and a fragmented ontological understanding of what constitutes an effective prompt due to its relatively recent emergence. We establish a structured understanding of prompt engineering by assembling a taxonomy of prompting techniques and analyzing their applications. We present a detailed vocabulary of 33 vocabulary terms, a taxonomy of 58 LLM prompting techniques, and 40 techniques for other modalities. Additionally, we provide best practices and guidelines for prompt engineering, including advice for prompting state-of-the-art (SOTA) LLMs such as ChatGPT. We further present a meta-analysis of the entire literature on natural language prefix-prompting. As a culmination of these efforts, this paper presents the most comprehensive survey on prompt engineering to date.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Costa, Hevander Da and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
	month = feb,
	year = {2025},
	note = {arXiv:2406.06608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/RNZ2YDMZ/Schulhoff et al. - 2025 - The Prompt Report A Systematic Survey of Prompt Engineering Techniques.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/GAZT6TNH/2406.html:text/html},
}

@article{hakkarainen_interrogative_2002-2,
	title = {The {Interrogative} {Model} of {Inquiry} and {Computer}-{Supported} {Collaborative} {Learning}},
	volume = {11},
	issn = {1573-1901},
	url = {https://doi.org/10.1023/A:1013076706416},
	doi = {10.1023/A:1013076706416},
	abstract = {The purpose of the study was to examine how the Interrogative Modelof Inquiry (I-Model), developed by Jaakko Hintikka and Matti Sintonenfor the purposes of epistemology and philosophy of science, could be applied to analyze elementary schoolstudents' process of inquiry in computer-supported learning. We review the basic assumptions of I-Model,report results of empirical investigation of the model in the context of computer-supportedcollaborative learning, and discuss pedagogical implications of the model. The results of the studyfurnished evidence that elementary school students were able to transform initially vagueexplanation-seeking question to a series of more specific subordinate questions while pursuing theirknowledge-seeking inquiry. The evidence presented indicates that, in an appropriate environment, it is entirelypossible for young students, with computer-supportfor collaborative learning, to engage in sophisticatedknowledge seeking analogous to scientific inquiry. We argue that the interrogative approach to inquiry canproductively be applied for conceptualizing inquiry in the context of computer-supported learning.},
	language = {en},
	number = {1},
	urldate = {2025-08-13},
	journal = {Science \& Education},
	author = {Hakkarainen, Kai and Sintonen, Matti},
	month = jan,
	year = {2002},
	keywords = {Basic Assumption, Collaborative Learn, Elementary School, Empirical Investigation, Scientific Inquiry},
	pages = {25--43},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/3BQZJG69/Hakkarainen and Sintonen - 2002 - The Interrogative Model of Inquiry and Computer-Supported Collaborative Learning.pdf:application/pdf},
}

@article{morucci_model_2024,
	title = {Model {Complexity} for {Supervised} {Learning}: {Why} {Simple} {Models} {Almost} {Always} {Work} {Best}, {And} {Why} {It} {Matters} for {Applied} {Research}},
	url = {https://arthurspirling.org/documents/MorucciSpirling_JustDoOLS.pdf},
	abstract = {Inspired by other fields, political scientists have embraced the use of supervised learning for prediction, inference, measurement and description. In doing so, they typically use flexible models of considerable complexity that have proved successful in non-social science settings. Yet there appear to be profound limits to the payoff of such approaches, at least relative to the alternative of using very simple (generalized linear) models for such tasks. We explain why this is, how to identify the problems for which this will be true, and what to do about it. We show that the intrinsic dimension of political science data is low, and this means returns to complexity are muted or non-existant. We provide a theory of “data curation” to explain this state of affairs. Our approach allows us to diagnose when simple models are optimal, and to provide advice for practitioners seeking to use machine learning.},
	language = {en},
	journal = {Spirling, Arthur},
	author = {Morucci, Marco},
	year = {2024},
	file = {PDF:/Users/carolinewagner/Zotero/storage/H4PIKGN4/Morucci - Model Complexity for Supervised Learning Why Simple Models Almost Always Work Best, And Why It Matt.pdf:application/pdf},
}

@article{barrie_replication_2025,
	title = {Replication for {Language} {Models}},
	url = {https://arthurspirling.org/documents/BarriePalmerSpirling_TrustMeBro.pdf},
	abstract = {Large Language Models (LMs) are exciting tools: they require minimal researcher input and but make it possible to annotate and generate large quantities of data. Yet there has been almost no systematic research into the reproducibility of research using LMs. This is a potential problem for scientific integrity. We give a theoretical framework for replication in the discipline and show that LM work is perhaps uniquely problematic. We demonstrate the problem empirically using a rolling iterated replication design in which we compare crowdsourcing and LMs on multiple repeated tasks, over many months. We find that LMs can be accurate, but the observed variance in performance is often unacceptably high. Strict “temperature” control does not resolve these issues. This affects downstream results. In many cases the LM findings cannot be re-run, let alone replicated. We conclude with recommendations for best practice, including the use of locally versioned ‘open’ LMs.},
	language = {en},
	author = {Barrie, Christopher and Palmer, Alexis and Spirling, Arthur},
	year = {2025},
	file = {PDF:/Users/carolinewagner/Zotero/storage/KXH4537G/Barrie et al. - Replication for Language Models.pdf:application/pdf},
}

@article{scorzato_reliability_2024,
	title = {Reliability and {Interpretability} in {Science} and {Deep} {Learning}},
	volume = {34},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/s11023-024-09682-0},
	doi = {10.1007/s11023-024-09682-0},
	abstract = {In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models—and in particular Deep Neural Network (DNN) models—which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown to be language-independent. It is argued that the high epistemic complexity of DNN models hinders the estimate of their reliability and also their prospect of long term progress. Some potential ways forward are suggested. Thirdly, this article identifies the close relation between a model’s epistemic complexity and its interpretability, as introduced in the context of responsible AI. This clarifies in which sense—and to what extent—the lack of understanding of a model (black-box problem) impacts its interpretability in a way that is independent of individual skills. It also clarifies how interpretability is a precondition for a plausible assessment of the reliability of any model, which cannot be based on statistical analysis alone. This article focuses on the comparison between traditional scientific models and DNN models. However, Random Forest (RF) and Logistic Regression (LR) models are also briefly considered.},
	language = {en},
	number = {3},
	urldate = {2025-08-13},
	journal = {Minds and Machines},
	author = {Scorzato, Luigi},
	month = jun,
	year = {2024},
	keywords = {Artificial intelligence, Complexity, Deep neural network, Interpretability, Philosophy of science, Reliability},
	pages = {27},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/69WTA62E/Scorzato - 2024 - Reliability and Interpretability in Science and Deep Learning.pdf:application/pdf},
}

@article{egami_using_2024,
	title = {Using {Large} {Language} {Model} {Annotations} for the {Social} {Sciences}: {A} {General} {Framework} of {Using} {Predicted} {Variables} in {Downstream} {Analyses}},
	url = {https://naokiegami.com/paper/dsl_ss.pdf},
	abstract = {Social scientists use automated annotation methods, such as supervised machine learning and, more recently, large language models (LLMs), that can predict labels and generate text-based variables. While such predicted text-based variables are often analyzed as if they were observed without errors, we show that ignoring prediction errors in the automated annotation step leads to substantial bias and invalid confidence intervals in downstream analyses, even if the accuracy of the automated annotations is high, e.g., above 90\%. We propose a framework of design-based supervised learning (DSL) that can provide valid statistical estimates, even when predicted variables contain non-random prediction errors. DSL employs a doubly robust procedure to combine predicted labels and a smaller number of expert annotations. DSL allows scholars to apply advances in LLMs to social science research while maintaining statistical validity. We illustrate its general applicability using two applications where the outcome and independent variables are text-based.},
	language = {en},
	author = {Egami, Naoki and Hinck, Musashi and Stewart, Brandon M and Wei, Hanying},
	year = {2024},
	file = {PDF:/Users/carolinewagner/Zotero/storage/5L5852MC/Egami et al. - Using Large Language Model Annotations for the Social Sciences A General Framework of Using Predict.pdf:application/pdf},
}

@misc{huang_survey_2024,
	title = {A {Survey} of {Uncertainty} {Estimation} in {LLMs}: {Theory} {Meets} {Practice}},
	shorttitle = {A {Survey} of {Uncertainty} {Estimation} in {LLMs}},
	url = {http://arxiv.org/abs/2410.15326},
	doi = {10.48550/arXiv.2410.15326},
	abstract = {As large language models (LLMs) continue to evolve, understanding and quantifying the uncertainty in their predictions is critical for enhancing application credibility. However, the existing literature relevant to LLM uncertainty estimation often relies on heuristic approaches, lacking systematic classification of the methods. In this survey, we clarify the definitions of uncertainty and confidence, highlighting their distinctions and implications for model predictions. On this basis, we integrate theoretical perspectives, including Bayesian inference, information theory, and ensemble strategies, to categorize various classes of uncertainty estimation methods derived from heuristic approaches. Additionally, we address challenges that arise when applying these methods to LLMs. We also explore techniques for incorporating uncertainty into diverse applications, including out-of-distribution detection, data annotation, and question clarification. Our review provides insights into uncertainty estimation from both definitional and theoretical angles, contributing to a comprehensive understanding of this critical aspect in LLMs. We aim to inspire the development of more reliable and effective uncertainty estimation approaches for LLMs in real-world scenarios.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Huang, Hsiu-Yuan and Yang, Yutong and Zhang, Zhaoxi and Lee, Sanwoo and Wu, Yunfang},
	month = oct,
	year = {2024},
	note = {arXiv:2410.15326 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/ULJ2PT35/Huang et al. - 2024 - A Survey of Uncertainty Estimation in LLMs Theory Meets Practice.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/MMKDZ95Q/2410.html:text/html},
}

@misc{wang_aleatoric_2025,
	title = {From {Aleatoric} to {Epistemic}: {Exploring} {Uncertainty} {Quantification} {Techniques} in {Artificial} {Intelligence}},
	shorttitle = {From {Aleatoric} to {Epistemic}},
	url = {http://arxiv.org/abs/2501.03282},
	doi = {10.48550/arXiv.2501.03282},
	abstract = {Uncertainty quantification (UQ) is a critical aspect of artificial intelligence (AI) systems, particularly in high-risk domains such as healthcare, autonomous systems, and financial technology, where decision-making processes must account for uncertainty. This review explores the evolution of uncertainty quantification techniques in AI, distinguishing between aleatoric and epistemic uncertainties, and discusses the mathematical foundations and methods used to quantify these uncertainties. We provide an overview of advanced techniques, including probabilistic methods, ensemble learning, sampling-based approaches, and generative models, while also highlighting hybrid approaches that integrate domain-specific knowledge. Furthermore, we examine the diverse applications of UQ across various fields, emphasizing its impact on decision-making, predictive accuracy, and system robustness. The review also addresses key challenges such as scalability, efficiency, and integration with explainable AI, and outlines future directions for research in this rapidly developing area. Through this comprehensive survey, we aim to provide a deeper understanding of UQ's role in enhancing the reliability, safety, and trustworthiness of AI systems.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Wang, Tianyang and Wang, Yunze and Zhou, Jun and Peng, Benji and Song, Xinyuan and Zhang, Charles and Sun, Xintian and Niu, Qian and Liu, Junyu and Chen, Silin and Chen, Keyu and Li, Ming and Feng, Pohsun and Bi, Ziqian and Liu, Ming and Zhang, Yichao and Fei, Cheng and Yin, Caitlyn Heqi and Yan, Lawrence KQ},
	month = jan,
	year = {2025},
	note = {arXiv:2501.03282 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/QUQVAB46/Wang et al. - 2025 - From Aleatoric to Epistemic Exploring Uncertainty Quantification Techniques in Artificial Intellige.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/VP3VKKCX/2501.html:text/html},
}

@misc{gawlikowski_survey_2022,
	title = {A {Survey} of {Uncertainty} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2107.03342},
	doi = {10.48550/arXiv.2107.03342},
	abstract = {Due to their increasing spread, confidence in neural network predictions became more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over or under confidence. Many researchers have been working on understanding and quantifying uncertainty in a neural network's prediction. As a result, different types and sources of uncertainty have been identified and a variety of approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. A comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and not reducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks, ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for the calibration of neural networks and give an overview of existing baselines and implementations. Different examples from the wide spectrum of challenges in different fields give an idea of the needs and challenges regarding uncertainties in practical applications. Additionally, the practical limitations of current methods for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao Xiang},
	month = jan,
	year = {2022},
	note = {arXiv:2107.03342 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/CAQSQBCN/Gawlikowski et al. - 2022 - A Survey of Uncertainty in Deep Neural Networks.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/K6MR55GA/2107.html:text/html},
}

@misc{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	url = {http://arxiv.org/abs/1506.02142},
	doi = {10.48550/arXiv.1506.02142},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = oct,
	year = {2016},
	note = {arXiv:1506.02142 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/75DH6874/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/95YLVY58/1506.html:text/html},
}

@inproceedings{de_vassimon_manela_stereotype_2021,
	address = {Online},
	title = {Stereotype and {Skew}: {Quantifying} {Gender} {Bias} in {Pre}-trained and {Fine}-tuned {Language} {Models}},
	shorttitle = {Stereotype and {Skew}},
	url = {https://aclanthology.org/2021.eacl-main.190/},
	doi = {10.18653/v1/2021.eacl-main.190},
	abstract = {This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.},
	urldate = {2025-08-13},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {de Vassimon Manela, Daniel and Errington, David and Fisher, Thomas and van Breugel, Boris and Minervini, Pasquale},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	month = apr,
	year = {2021},
	pages = {2232--2242},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/7ZLDWRXK/de Vassimon Manela et al. - 2021 - Stereotype and Skew Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models.pdf:application/pdf},
}

@article{shen_experimental_2023,
	title = {An experimental study measuring the generalization of fine-tuned language representation models across commonsense reasoning benchmarks},
	volume = {40},
	copyright = {© 2023 The Authors. Expert Systems published by John Wiley \& Sons Ltd.},
	issn = {1468-0394},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13243},
	doi = {10.1111/exsy.13243},
	abstract = {In the last 5 years, language representation models, such as BERT and GPT-3, based on transformer neural networks, have led to enormous progress in natural language processing (NLP). One such NLP task is commonsense reasoning, where performance is usually evaluated through multiple-choice question answering benchmarks. Till date, many such benchmarks have been proposed, and ‘leaderboards’ tracking state-of-the-art performance on those benchmarks suggest that transformer-based models are approaching human-like performance. Because these are commonsense benchmarks, however, such a model should be expected to generalize, that is, at least in aggregate, should not exhibit excessive performance loss across independent commonsense benchmarks regardless of the specific benchmark on (the training set of) which it has been fine-tuned. In this article, we evaluate this expectation by proposing a methodology and experimental study to measure the generalization ability of language representation models using a rigorous and intuitive metric. Using five established commonsense reasoning benchmarks, our experimental study shows that the models do not generalize well, and may be (potentially) susceptible to issues such as dataset bias. The results therefore suggest that current performance on benchmarks may be an over-estimate, especially if we want to use such models on novel commonsense problems for which a ‘training’ dataset may not be available, for the language representation model, to fine-tune on.},
	language = {en},
	number = {5},
	urldate = {2025-08-13},
	journal = {Expert Systems},
	author = {Shen, Ke and Kejriwal, Mayank},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.13243},
	keywords = {commonsense reasoning, generalization, language representation model},
	pages = {e13243},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/Y2N836IM/Shen and Kejriwal - 2023 - An experimental study measuring the generalization of fine-tuned language representation models acro.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/D6V4WF7S/exsy.html:text/html},
}

@article{massing_degrees_2017,
	title = {Degrees of competency: the relationship between educational qualifications and adult skills across countries},
	volume = {5},
	issn = {2196-0739},
	shorttitle = {Degrees of competency},
	url = {https://doi.org/10.1186/s40536-017-0041-y},
	doi = {10.1186/s40536-017-0041-y},
	abstract = {Educational qualifications and literacy skills are highly related. This is not surprising as it is one aim of educational systems to equip individuals with competencies necessary to take part in society. Because of this relationship educational qualifications are often used as a proxy for “human capital”. However, from a theoretical perspective, there are many reasons why this relationship is not perfect, and to some degree this is due to third variables. Thus, we want to explore the net relationship between educational attainment (harmonized according to the International Standard Classification of Education, ISCED) and literacy skills, and how much skills vary within education levels across countries.},
	number = {1},
	urldate = {2025-08-13},
	journal = {Large-scale Assessments in Education},
	author = {Massing, Natascha and Schneider, Silke L.},
	month = feb,
	year = {2017},
	keywords = {Education Category, Educational Attainment, Educational Qualification, Literacy Skill, Migration Background},
	pages = {6},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/692ZCJYB/Massing and Schneider - 2017 - Degrees of competency the relationship between educational qualifications and adult skills across c.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/YABMMSHI/s40536-017-0041-y.html:text/html},
}

@article{pereira_language_2022,
	title = {Language skills differences between adults without formal education and low formal education},
	volume = {35},
	issn = {1678-7153},
	url = {https://doi.org/10.1186/s41155-021-00205-9},
	doi = {10.1186/s41155-021-00205-9},
	abstract = {The influence of education on cognition has been extensively researched, particularly in countries with high levels of illiteracy. However, the impact of low education in all cognitive functions appears to differ. Regarding to language, the effects of education on many linguistic tasks—supported by different processing—remain unclear. The primary objective of this study was to determine whether oral language task performance differs among individuals with no formal and low-educated subjects, as measured by the Brazilian Montreal-Toulouse Language Assessment Battery (MTL-BR). This is the only language battery available for use in Brazil, but lacks normative data for illiterate individuals. The secondary objective was to gather data for use as clinical parameters in assessing persons with aphasia (PWA) not exposed to a formal education.},
	number = {1},
	urldate = {2025-08-13},
	journal = {Psicologia: Reflexão e Crítica},
	author = {Pereira, Ariane and Ortiz, Karin Zazo},
	month = jan,
	year = {2022},
	keywords = {Aphasia, Educational status, Language, Language disorders, Neuropsychological tests},
	pages = {4},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/C9RSK2BQ/Pereira and Ortiz - 2022 - Language skills differences between adults without formal education and low formal education.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/2DK5V32Z/s41155-021-00205-9.html:text/html},
}

@article{panzeri_childrens_2021-1,
	title = {Children’s and {Adults}’ {Sensitivity} to {Gricean} {Maxims} and to the {Maximize} {Presupposition} {Principle}},
	volume = {12},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2021.624628/full},
	doi = {10.3389/fpsyg.2021.624628},
	abstract = {Up to age five, children are known to experience difficulties in the derivation of implicitly conveyed content, sticking to literally true, even if underinformative, interpretation of sentences. The computation of implicated meanings is connected to the (apparent or manifest) violation of Gricean conversational maxims. We present a study that tests unmotivated violations of the maxims of Quantity, Relevance and Manner, and of the Maximize Presupposition principle, with a Truth Value Judgment task with three options of response. We tested preschoolers and school-aged children, with adults as controls, to verify at which age these pragmatic rules are recognized, and to see whether there is a difference among these tenets. We found an evolutionary trend, and that in all age groups violations of the maxims of Quantity and of Relation are sanctioned to a higher degree compared to infringements of the Maim of Manner and of the Maximize Presupposition principle. We conjecture that this relates to the effects that the violation of a certain maxim or principle has on the goals of the exchange: listeners are less tolerant with statements that transmit inaccurate or incomplete information, while being more tolerant with those that still permit to understand what has happened.},
	language = {English},
	urldate = {2025-08-13},
	journal = {Frontiers in Psychology},
	author = {Panzeri, Francesca and Foppolo, Francesca},
	month = mar,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {Acquisition of pragmatics, Experimental pragmatics, Maxim of manner, maximize presuppositions, Maxims of conversation, Pragmatic tolerance},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/WCVM8T3Q/Panzeri and Foppolo - 2021 - Children’s and Adults’ Sensitivity to Gricean Maxims and to the Maximize Presupposition Principle.pdf:application/pdf},
}

@inproceedings{zheng_grice_2021,
	address = {Online},
	title = {{GRICE}: {A} {Grammar}-based {Dataset} for {Recovering} {Implicature} and {Conversational} {rEasoning}},
	shorttitle = {{GRICE}},
	url = {https://aclanthology.org/2021.findings-acl.182/},
	doi = {10.18653/v1/2021.findings-acl.182},
	urldate = {2025-08-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Zheng, Zilong and Qiu, Shuwen and Fan, Lifeng and Zhu, Yixin and Zhu, Song-Chun},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {2074--2085},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/5994SVKB/Zheng et al. - 2021 - GRICE A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning.pdf:application/pdf},
}

@article{viveros-munoz_does_2025,
	title = {Does the {Grammatical} {Structure} of {Prompts} {Influence} the {Responses} of {Generative} {Artificial} {Intelligence}? {An} {Exploratory} {Analysis} in {Spanish}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Does the {Grammatical} {Structure} of {Prompts} {Influence} the {Responses} of {Generative} {Artificial} {Intelligence}?},
	url = {https://www.mdpi.com/2076-3417/15/7/3882},
	doi = {10.3390/app15073882},
	abstract = {Generative Artificial Intelligence (AI) has transformed personal and professional domains by enabling creative content generation and problem-solving. However, the influence of users’ grammatical abilities on AI-generated responses remains unclear. This exploratory study examines how language and grammar abilities in Spanish affect the quality of responses from ChatGPT (version 3.5). Despite the robust performance of Large Language Models (LLMs) in various tasks, they face challenges with grammatical moods specific to non-English languages, such as the subjunctive in Spanish. Higher education students were chosen as participants due to their familiarity with AI and its potential use in learning. The study assessed ChatGPT’s ability to process instructions in Chilean Spanish, analyzing how linguistic complexity, grammatical variations, and informal language impacted output quality. The results indicate that varied verbal moods and complex sentence structures significantly influence prompt evaluation, response quality, and response length. Based on these findings, a framework is proposed to guide higher education communities in promoting digital literacy and integrating AI into teaching and learning.},
	language = {en},
	number = {7},
	urldate = {2025-08-13},
	journal = {Applied Sciences},
	author = {Viveros-Muñoz, Rhoddy and Carrasco-Sáez, José and Contreras-Saavedra, Carolina and San-Martín-Quiroga, Sheny and Contreras-Saavedra, Carla E.},
	month = jan,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI in education, generative AI, natural language processing, prompt engineering, Spanish grammar performance},
	pages = {3882},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/ERZ2YGGZ/Viveros-Muñoz et al. - 2025 - Does the Grammatical Structure of Prompts Influence the Responses of Generative Artificial Intellige.pdf:application/pdf},
}

@misc{kharchenko_how_2025,
	title = {How {Well} {Do} {LLMs} {Represent} {Values} {Across} {Cultures}? {Empirical} {Analysis} of {LLM} {Responses} {Based} on {Hofstede} {Cultural} {Dimensions}},
	shorttitle = {How {Well} {Do} {LLMs} {Represent} {Values} {Across} {Cultures}?},
	url = {http://arxiv.org/abs/2406.14805},
	doi = {10.48550/arXiv.2406.14805},
	abstract = {Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs' cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Kharchenko, Julia and Roosta, Tanya and Chadha, Aman and Shah, Chirag},
	month = aug,
	year = {2025},
	note = {arXiv:2406.14805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/QFDNQIIJ/Kharchenko et al. - 2025 - How Well Do LLMs Represent Values Across Cultures Empirical Analysis of LLM Responses Based on Hofs.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/FK376PLG/2406.html:text/html},
}

@misc{yin_should_2024,
	title = {Should {We} {Respect} {LLMs}? {A} {Cross}-{Lingual} {Study} on the {Influence} of {Prompt} {Politeness} on {LLM} {Performance}},
	shorttitle = {Should {We} {Respect} {LLMs}?},
	url = {http://arxiv.org/abs/2402.14531},
	doi = {10.48550/arXiv.2402.14531},
	abstract = {We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs). Polite language in human communications often garners more compliance and effectiveness, while rudeness can cause aversion, impacting response quality. We consider that LLMs mirror human communication traits, suggesting they align with human cultural norms. We assess the impact of politeness in prompts on LLMs across English, Chinese, and Japanese tasks. We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language. This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts. Our findings highlight the need to factor in politeness for cross-cultural natural language processing and LLM usage.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Yin, Ziqi and Wang, Hao and Horio, Kaito and Kawahara, Daisuke and Sekine, Satoshi},
	month = oct,
	year = {2024},
	note = {arXiv:2402.14531 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/IU9CL99P/Yin et al. - 2024 - Should We Respect LLMs A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performan.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/W26KE2EC/2402.html:text/html},
}

@article{novoa_generically_2023,
	title = {Generically partisan: {Polarization} in political communication},
	volume = {120},
	shorttitle = {Generically partisan},
	url = {https://www.pnas.org/doi/10.1073/pnas.2309361120},
	doi = {10.1073/pnas.2309361120},
	abstract = {American political parties continue to grow more polarized, but the extent of ideological polarization among the public is much less than the extent of perceived polarization (what the ideological gap is believed to be). Perceived polarization is concerning because of its link to interparty hostility, but it remains unclear what drives this phenomenon. We propose that a tendency for individuals to form broad generalizations about groups on the basis of inconsistent evidence may be partly responsible. We study this tendency by measuring the interpretation, endorsement, and recall of category-referring statements, also known as generics (e.g., “Democrats favor affirmative action”). In study 1 (n = 417), perceived polarization was substantially greater than actual polarization. Further, participants endorsed generics as long as they were true more often of the target party (e.g., Democrats favor affirmative action) than of the opposing party (e.g., Republicans favor affirmative action), even when they believed such statements to be true for well below 50\% of the relevant party. Study 2 (n = 928) found that upon receiving information from political elites, people tended to recall these statements as generic, regardless of whether the original statement was generic or not. Study 3 (n = 422) found that generic statements regarding new political information led to polarized judgments and did so more than nongeneric statements. Altogether, the data indicate a tendency toward holding mental representations of political claims that exaggerate party differences. These findings suggest that the use of generic language, common in everyday speech, enables inferential errors that exacerbate perceived polarization.},
	number = {47},
	urldate = {2025-08-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Novoa, Gustavo and Echelbarger, Margaret and Gelman, Andrew and Gelman, Susan A.},
	month = nov,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2309361120},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/XVWPGIXE/Novoa et al. - 2023 - Generically partisan Polarization in political communication.pdf:application/pdf},
}

@article{kyrychenko_profiling_2025-1,
	title = {Profiling misinformation susceptibility},
	volume = {241},
	issn = {0191-8869},
	url = {https://www.sciencedirect.com/science/article/pii/S0191886925001394},
	doi = {10.1016/j.paid.2025.113177},
	abstract = {The global spread of misinformation poses a serious threat to the functioning of societies worldwide. But who falls for it? In this study, 66,242 individuals from 24 countries completed the Misinformation Susceptibility Test (MIST) and indicated their self-perceived misinformation discernment ability. Multilevel modelling showed that Generation Z, non-male, less educated, and more conservative individuals were more vulnerable to misinformation. Furthermore, while individuals' confidence in detecting misinformation was generally associated with better actual discernment, the degree to which perceived ability matched actual ability varied across subgroups. That is, whereas women were especially accurate in assessing their ability, extreme conservatives' perceived ability showed little relation to their actual misinformation discernment. Meanwhile, across all generations, Gen Z perceived their misinformation discernment ability most accurately, despite performing worst on the test. Taken together, our analyses provide the first systematic and holistic profile of misinformation susceptibility.},
	urldate = {2025-08-13},
	journal = {Personality and Individual Differences},
	author = {Kyrychenko, Yara and Koo, Hyunjin J. and Maertens, Rakoen and Roozenbeek, Jon and van der Linden, Sander and Götz, Friedrich M.},
	month = jul,
	year = {2025},
	keywords = {Conspiracy theories, Fake news, Misinformation susceptibility, MIST},
	pages = {113177},
	file = {ScienceDirect Full Text PDF:/Users/carolinewagner/Zotero/storage/4EN4ZYLE/Kyrychenko et al. - 2025 - Profiling misinformation susceptibility.pdf:application/pdf;ScienceDirect Snapshot:/Users/carolinewagner/Zotero/storage/2RHCEIZI/S0191886925001394.html:text/html},
}

@inproceedings{rottger_two_2022-2,
	address = {Seattle, United States},
	title = {Two {Contrasting} {Data} {Annotation} {Paradigms} for {Subjective} {NLP} {Tasks}},
	url = {https://aclanthology.org/2022.naacl-main.13},
	doi = {10.18653/v1/2022.naacl-main.13},
	abstract = {Labelled data is the foundation of most natural language processing tasks. However, labelling data is difﬁcult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss beneﬁts and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.},
	language = {en},
	urldate = {2025-07-16},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Röttger, Paul and Vidgen, Bertie and Hovy, Dirk and Pierrehumbert, Janet},
	year = {2022},
	file = {PDF:/Users/carolinewagner/Zotero/storage/4QZJVPMB/Rottger et al. - 2022 - Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks.pdf:application/pdf},
}

@article{laux_how_nodate-1,
	title = {How {Clear} {Rules} and {Higher} {Pay} {Increase} {Performance} in {Data} {Annotation} in the {AI} {Economy}},
	abstract = {The global surge in AI applications is transforming industries, leading to displacement and complementation of existing jobs, while also giving rise to new employment opportunities. Data annotation, encompassing the labelling of images or annotating of texts by human workers, crucially influences the quality of a dataset directly influences the quality of AI models trained on it. This paper delves into the economics of data annotation, with a specific focus on the impact of task instruction design (that is, the choice between rules and standards as theorised in law and economics) and monetary incentives on data quality and costs. An experimental study involving 307 data annotators examines six groups with varying task instructions (norms) and monetary incentives. Results reveal that annotators provided with clear rules exhibit higher accuracy rates, outperforming those with vague standards by 14\%. Similarly, annotators receiving an additional monetary incentive perform significantly better, with the highest accuracy rate recorded in the group working with both clear rules and incentives (87.5\% accuracy). In addition, our results show that rules are perceived as being more helpful by annotators than standards and reduce annotators’ difficulty in annotating images. These empirical findings underscore the double benefit of rule-based instructions on both data quality and worker wellbeing. Our research design allows us to reveal that, in our study, rules are more cost-efficient in increasing accuracy than monetary incentives. The paper contributes experimental insights to discussions on the economical, ethical, and legal considerations of AI technologies. Addressing policymakers and practitioners, we emphasise the need for a balanced approach in optimising data annotation processes for efficient and ethical AI development and usage.},
	language = {en},
	author = {Laux, Johann and Stephany, Fabian and Liefgreen, Alice},
	file = {PDF:/Users/carolinewagner/Zotero/storage/2D4KMEJL/Laux et al. - How Clear Rules and Higher Pay Increase Performance in Data Annotation in the AI Economy.pdf:application/pdf},
}

@article{cohen_kadosh_high_2015-4,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Classical, Conditioning, Reflex, Startle},
	pages = {50--57},
}

@article{cohen_kadosh_high_2015-5,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Classical, Conditioning, Reflex, Startle},
	pages = {50--57},
	file = {Accepted Version:/Users/carolinewagner/Zotero/storage/WHQYPMSY/Cohen Kadosh et al. - 2015 - High trait anxiety during adolescence interferes w.pdf:application/pdf},
}

@article{cohen_kadosh_high_2015-6,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Classical, Conditioning, Reflex, Startle},
	pages = {50--57},
}

@article{cohen_kadosh_high_2015-7,
	title = {High trait anxiety during adolescence interferes with discriminatory context learning},
	volume = {123},
	issn = {1095-9564},
	doi = {10.1016/j.nlm.2015.05.002},
	abstract = {Persistent adult anxiety disorders often begin in adolescence. As emphasis on early treatment grows, we need a better understanding of how adolescent anxiety develops. In the current study, we used a fear conditioning paradigm to identify disruptions in cue and context threat-learning in 19 high anxious (HA) and 24 low anxious (LA) adolescents (12-17years). We presented three neutral female faces (conditioned stimulus, CS) in three contingent relations with an unconditioned stimulus (UCS, a shrieking female scream) in three virtual room contexts. The degree of contingency between the CSs and the UCSs varied across the rooms: in the predictable scream condition, the scream followed the face on 100\% of trials; in the unpredictable scream condition, the scream and face appeared randomly and independently of each other; in the no-scream condition the CS was presented in the absence of any UCS. We found that the LA adolescents showed higher levels of fear-potentiated startle to the faces relative to the rooms. This difference was independent of the contingency condition. The HA adolescents showed non-differential startle between the CSs, but, in contrast to previous adult data, across both cue types displayed lowest startle to the unpredictable condition and highest startle to the no-scream condition. Our study is the first to examine context conditioning in adolescents, and our results suggest that high trait anxiety early in development may be associated with an inability to disambiguate the signalling roles of cues and contexts, and a mislabelling of safety or ambiguous signals.},
	language = {eng},
	journal = {Neurobiology of Learning and Memory},
	author = {Cohen Kadosh, Kathrin and Haddad, Anneke D. M. and Heathcote, Lauren C. and Murphy, Robin A. and Pine, Daniel S. and Lau, Jennifer Y. F.},
	month = sep,
	year = {2015},
	pmid = {25982943},
	pmcid = {PMC6309538},
	keywords = {Adolescence, Adolescent, Adolescent Development, Anxiety, Child, Context conditioning, Cues, Development, Fear, Female, Humans, Individual differences, Individuality, Insp Res, Learning, Male, Classical, Conditioning, Reflex, Startle},
	pages = {50--57},
	file = {Accepted Version:/Users/carolinewagner/Zotero/storage/UU7BDR4L/Cohen Kadosh et al. - 2015 - High trait anxiety during adolescence interferes w.pdf:application/pdf},
}

@misc{laux_improving_2023-2,
	title = {Improving {Task} {Instructions} for {Data} {Annotators}: {How} {Clear} {Rules} and {Higher} {Pay} {Increase} {Performance} in {Data} {Annotation} in the {AI} {Economy}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Improving {Task} {Instructions} for {Data} {Annotators}},
	url = {https://arxiv.org/abs/2312.14565},
	abstract = {The global surge in AI applications is transforming industries, leading to displacement and complementation of existing jobs, while also giving rise to new employment opportunities. Data annotation, encompassing the labelling of images or annotating of texts by human workers, crucially influences the quality of a dataset directly influences the quality of AI models trained on it. This paper delves into the economics of data annotation, with a specific focus on the impact of task instruction design (that is, the choice between rules and standards as theorised in law and economics) and monetary incentives on data quality and costs. An experimental study involving 307 data annotators examines six groups with varying task instructions (norms) and monetary incentives. Results reveal that annotators provided with clear rules exhibit higher accuracy rates, outperforming those with vague standards by 14\%. Similarly, annotators receiving an additional monetary incentive perform significantly better, with the highest accuracy rate recorded in the group working with both clear rules and incentives (87.5\% accuracy). In addition, our results show that rules are perceived as being more helpful by annotators than standards and reduce annotators' difficulty in annotating images. These empirical findings underscore the double benefit of rule-based instructions on both data quality and worker wellbeing. Our research design allows us to reveal that, in our study, rules are more cost-efficient in increasing accuracy than monetary incentives. The paper contributes experimental insights to discussions on the economical, ethical, and legal considerations of AI technologies. Addressing policymakers and practitioners, we emphasise the need for a balanced approach in optimising data annotation processes for efficient and ethical AI development and usage.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Laux, Johann and Stephany, Fabian and Liefgreen, Alice},
	year = {2023},
	doi = {10.48550/ARXIV.2312.14565},
	keywords = {Applications (stat.AP), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Economics and business, General Economics (econ.GN)},
}

@book{grice_studies_1991-2,
	address = {Cambridge (Mass.) London},
	title = {Studies in the way of words},
	isbn = {978-0-674-85271-6},
	language = {eng},
	publisher = {Harvard university press},
	author = {Grice, H. Paul},
	year = {1991},
}

@article{kasirzadeh_conversation_2023-3,
	title = {In {Conversation} with {Artificial} {Intelligence}: {Aligning} language {Models} with {Human} {Values}},
	volume = {36},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2210-5433, 2210-5441},
	shorttitle = {In {Conversation} with {Artificial} {Intelligence}},
	url = {https://link.springer.com/10.1007/s13347-023-00606-x},
	doi = {10.1007/s13347-023-00606-x},
	abstract = {AbstractLarge-scale language technologies are increasingly used in various forms of communication with humans across different contexts. One particular use case for these technologies is conversational agents, which output natural language text in response to prompts and queries. This mode of engagement raises a number of social and ethical questions. For example, what does it mean to align conversational agents with human norms or values? Which norms or values should they be aligned with? And how can this be accomplished? In this paper, we propose a number of steps that help answer these questions. We start by developing a philosophical analysis of the building blocks of linguistic communication between conversational agents and human interlocutors. We then use this analysis to identify and formulate ideal norms of conversation that can govern successful linguistic communication between humans and conversational agents. Furthermore, we explore how these norms can be used to align conversational agents with human values across a range of different discursive domains. We conclude by discussing the practical implications of our proposal for the design of conversational agents that are aligned with these norms and values.},
	language = {en},
	number = {2},
	urldate = {2025-07-16},
	journal = {Philosophy \& Technology},
	author = {Kasirzadeh, Atoosa and Gabriel, Iason},
	month = jun,
	year = {2023},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/6GYCNHWZ/Kasirzadeh and Gabriel - 2023 - In Conversation with Artificial Intelligence Aligning language Models with Human Values.pdf:application/pdf},
}

@misc{kim_applying_2025-2,
	title = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}: {Design} {Insights} from a {Participatory} {Approach}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}},
	url = {https://arxiv.org/abs/2503.00858},
	abstract = {While large language models (LLMs) are increasingly used to assist users in various tasks through natural language interactions, these interactions often fall short due to LLMs' limited ability to infer contextual nuances and user intentions, unlike humans. To address this challenge, we draw inspiration from the Gricean Maxims–human communication theory that suggests principles of effective communication–and aim to derive design insights for enhancing human-AI interactions (HAI). Through participatory design workshops with communication experts, designers, and end-users, we identified ways to apply these maxims across the stages of the HAI cycle. Our findings include reinterpreted maxims tailored to human-LLM contexts and nine actionable design considerations categorized by interaction stage. These insights provide a concrete framework for designing more cooperative and user-centered LLM-based systems, bridging theoretical foundations in communication with practical applications in HAI.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Kim, Yoonsu and Chin, Brandon and Son, Kihoon and Kim, Seoyoung and Kim, Juho},
	year = {2025},
	doi = {10.48550/ARXIV.2503.00858},
	keywords = {FOS: Computer and information sciences, Human-Computer Interaction (cs.HC)},
}

@misc{saad_gricean_2025-2,
	title = {Gricean {Norms} as a {Basis} for {Effective} {Collaboration}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2503.14484},
	abstract = {Effective human-AI collaboration hinges not only on the AI agent's ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks – common ground, relevance theory, and theory of mind – into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent's pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Saad, Fardin and Murukannaiah, Pradeep K. and Singh, Munindar P.},
	year = {2025},
	doi = {10.48550/ARXIV.2503.14484},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Computation and Language (cs.CL), Multiagent Systems (cs.MA)},
}

@inproceedings{krause_gricean_2024-1,
	address = {Tokyo, Japan},
	title = {The {Gricean} {Maxims} in {NLP} - {A} {Survey}},
	url = {https://aclanthology.org/2024.inlg-main.39},
	doi = {10.18653/v1/2024.inlg-main.39},
	urldate = {2025-07-16},
	booktitle = {Proceedings of the 17th {International} {Natural} {Language} {Generation} {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {Krause, Lea and Vossen, Piek T.J.M.},
	year = {2024},
	pages = {470--485},
}

@incollection{bloom_taxonomy_1986-1,
	address = {London},
	edition = {29. print},
	title = {Taxonomy of educational objectives.},
	isbn = {978-0-582-28010-6},
	shorttitle = {Taxonomy of educational objectives. 1},
	language = {eng},
	publisher = {Longman},
	author = {Bloom, Benjamin S.},
	year = {1986},
}

@misc{luo_enhanced_2025-2,
	title = {Enhanced {Bloom}'s {Educational} {Taxonomy} for {Fostering} {Information} {Literacy} in the {Era} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2503.19434},
	abstract = {The advent of Large Language Models (LLMs) has profoundly transformed the paradigms of information retrieval and problem-solving, enabling students to access information acquisition more efficiently to support learning. However, there is currently a lack of standardized evaluation frameworks that guide learners in effectively leveraging LLMs. This paper proposes an LLM-driven Bloom's Educational Taxonomy that aims to recognize and evaluate students' information literacy (IL) with LLMs, and to formalize and guide students practice-based activities of using LLMs to solve complex problems. The framework delineates the IL corresponding to the cognitive abilities required to use LLM into two distinct stages: Exploration \&amp; Action and Creation \&amp; Metacognition. It further subdivides these into seven phases: Perceiving, Searching, Reasoning, Interacting, Evaluating, Organizing, and Curating. Through the case presentation, the analysis demonstrates the framework's applicability and feasibility, supporting its role in fostering IL among students with varying levels of prior knowledge. This framework fills the existing gap in the analysis of LLM usage frameworks and provides theoretical support for guiding learners to improve IL.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Luo, Yiming and Liu, Ting and Pang, Patrick Cheong-Iao and McKay, Dana and Chen, Ziqi and Buchanan, George and Chang, Shanton},
	year = {2025},
	doi = {10.48550/ARXIV.2503.19434},
	keywords = {FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@misc{yaacoub_assessing_2025-2,
	title = {Assessing {AI}-{Generated} {Questions}' {Alignment} with {Cognitive} {Frameworks} in {Educational} {Assessment}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2504.14232},
	abstract = {This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz, an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured framework for categorizing educational objectives into hierarchical cognitive levels. Our research investigates whether incorporating this taxonomy can improve the alignment of AI-generated questions with specific cognitive objectives. We developed a dataset of 3691 questions categorized according to Bloom's levels and employed various classification models-Multinomial Logistic Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a Transformer-based model (DistilBERT)-to evaluate their effectiveness in categorizing questions. Our results indicate that higher Bloom's levels generally correlate with increased question length, Flesch-Kincaid Grade Level (FKGL), and Lexical Density (LD), reflecting the increased complexity of higher cognitive demands. Multinomial Logistic Regression showed varying accuracy across Bloom's levels, performing best for "Knowledge" and less accurately for higher-order levels. Merging higher-level categories improved accuracy for complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective classification for lower levels but struggled with higher-order tasks. DistilBERT achieved the highest performance, significantly improving classification of both lower and higher-order cognitive levels, achieving an overall validation accuracy of 91\%. This study highlights the potential of integrating Bloom's Taxonomy into AI-driven assessment tools and underscores the advantages of advanced models like DistilBERT for enhancing educational content generation.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Yaacoub, Antoun and Da-Rugna, Jérôme and Assaghir, Zainab},
	year = {2025},
	doi = {10.48550/ARXIV.2504.14232},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@article{elim_promoting_2024-2,
	title = {Promoting cognitive skills in {AI}-supported learning environments: the integration of bloom’s taxonomy},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {0300-4279, 1475-7575},
	shorttitle = {Promoting cognitive skills in {AI}-supported learning environments},
	url = {https://www.tandfonline.com/doi/full/10.1080/03004279.2024.2332469},
	doi = {10.1080/03004279.2024.2332469},
	language = {en},
	urldate = {2025-07-16},
	journal = {Education 3-13},
	author = {Elim, Emily Hui Sein Yue},
	month = apr,
	year = {2024},
	pages = {1--11},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/UPKMKFVC/Elim - 2024 - Promoting cognitive skills in AI-supported learning environments the integration of bloom’s taxonom.pdf:application/pdf},
}

@article{hakkarainen_no_2002-1,
	title = {[{No} title found]},
	volume = {11},
	issn = {0926-7220},
	url = {http://link.springer.com/10.1023/A:1013076706416},
	doi = {10.1023/a:1013076706416},
	number = {1},
	urldate = {2025-07-16},
	journal = {Science and Education},
	author = {Hakkarainen, Kai and Sintonen, Matti},
	year = {2002},
	pages = {25--43},
}

@article{hakkarainen_interrogative_2002-3,
	title = {The {Interrogative} {Model} of {Inquiry} and {Computer}-{Supported} {Collaborative} {Learning}},
	volume = {11},
	issn = {0926-7220},
	url = {http://link.springer.com/10.1023/A:1013076706416},
	doi = {10.1023/a:1013076706416},
	number = {1},
	urldate = {2025-07-16},
	journal = {Science and Education},
	author = {Hakkarainen, Kai and Sintonen, Matti},
	year = {2002},
	pages = {25--43},
}

@book{koralus_reason_2023-2,
	address = {Oxford},
	title = {Reason and {Inquiry}: {The} {Erotetic} {Theory}},
	isbn = {978-0-19-255701-8},
	shorttitle = {Reason and {Inquiry}},
	abstract = {Reason and Inquiry: The Erotetic Theory presents a unified theory of the human capacity for reasoning and decision-making. The book's central idea is that our minds naturally aim at resolving issues, and if we are sufficiently inquisitive in the process, we can avoid mistakes},
	language = {eng},
	publisher = {Oxford University Press, Incorporated},
	author = {Koralus, Philipp},
	year = {2023},
}

@misc{koralus_pyetr_2025-2,
	title = {{PyETR}},
	copyright = {© 2025 Philipp Koralus, Sean Moss, Mark Todd},
	publisher = {University of Oxford; University of Birmingham; Dreaming Spires},
	author = {Koralus, Philipp and Moss, Sean and Todd, Mark},
	year = {2025},
}

@article{harrah_nuel_1978-1,
	title = {Nuel {D}. {BelnapJr}., and {Thomas} {B}. {Steel} {Jr}. {The} logic of questions and answers. {Yale} {University} {Press}, {New} {Haven} and {London1976}, vii + 209 pp. - {Urs} {Egli} and {Hubert} {Schleichert}. {Bibliography} of the theory of questions and answers. {Therein}, pp. 155–200.},
	volume = {43},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0022-4812, 1943-5886},
	url = {https://www.cambridge.org/core/product/identifier/S0022481200049744/type/journal_article},
	doi = {10.2307/2272838},
	language = {en},
	number = {2},
	urldate = {2025-07-16},
	journal = {Journal of Symbolic Logic},
	author = {Harrah, David},
	month = jun,
	year = {1978},
	pages = {379--380},
}

@book{belnap_logic_1976-1,
	address = {New Haven},
	title = {The logic of questions and answers},
	isbn = {978-0-300-01962-9},
	publisher = {Yale University Press},
	author = {Belnap, Nuel D. and Steel, Thomas B.},
	year = {1976},
	keywords = {Formal languages, Question (Logic), Question-answering systems},
}

@article{kirk_prism_2024-2,
	title = {The {PRISM} {Alignment} {Dataset}: {What} {Participatory}, {Representative} and {Individualised} {Human} {Feedback} {Reveals} {About} the {Subjective} and {Multicultural} {Alignment} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {The {PRISM} {Alignment} {Dataset}},
	url = {https://arxiv.org/abs/2404.16019},
	doi = {10.48550/ARXIV.2404.16019},
	abstract = {Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.},
	urldate = {2025-07-16},
	author = {Kirk, Hannah Rose and Whitefield, Alexander and Röttger, Paul and Bean, Andrew and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
	year = {2024},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@misc{devlin_bert_2018-2,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BERT}},
	url = {https://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
}

@article{tran_investigation_2020-1,
	title = {An investigation into the flouting of conversational maxims employed by male and female guests in the {American} talk show "{The} {Ellen} {Show}"},
	issn = {1859-1531},
	url = {https://jst-ud.vn/jst-ud/article/view/3257},
	doi = {10.31130/jst-ud2020-069e},
	abstract = {Grice’s maxims are basic rules for interlocutors to follow in conversations. Nevertheless, when the speaker intentionally makes the hearer look for the real meaning beyond what is said implicitly, (s)he employs conversational maxim flouting. This article aims at investigating types of conversational maxim flouting and rhetorical strategies employed by male and female guests in the American talk show “The Ellen Show” and discovering similarities and differences in terms of the flouting of conversational maxims between two genders. The study design is based on a combination of qualitative and quantitative approaches and the application of descriptive and contrastive methods. The samples including 72 maxim flouting situations for each gender were selected from the Interviews section on the official channel of The Ellen Show. The findings reveal that although both genders share some similarities concerning the pragmatic features of maxim flouting, each gender reflects its own tendency of language style in communication.},
	urldate = {2025-07-24},
	journal = {Journal of Science and Technology Issue on Information and Communications Technology},
	author = {Tran, Nguyen Thi Quynh Hoa, Tran Thi Huyen Trang},
	month = jun,
	year = {2020},
	pages = {117--122},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/JENSD2RV/Tran - 2020 - An investigation into the flouting of conversational maxims employed by male and female gu.pdf:application/pdf},
}

@article{panzeri_childrens_2021-2,
	title = {Children’s and adults’ sensitivity to {Gricean} maxims and to the maximize presupposition principle},
	volume = {12},
	issn = {1664-1078},
	doi = {10.3389/fpsyg.2021.624628},
	abstract = {Up to age 5, children are known to experience difficulties in the derivation of implicitly conveyed content, sticking to literally true, even if underinformative, interpretation of sentences. The computation of implicated meanings is connected to the (apparent or manifest) violation of Gricean conversational maxims. We present a study that tests unmotivated violations of the maxims of Quantity, Relevance, and Manner and of the Maximize Presupposition principle, with a Truth Value Judgment task with three options of response. We tested pre-schoolers and school-aged children, with adults as controls, to verify at which age these pragmatic rules are recognized and to see whether there is a difference among these tenets. We found an evolutionary trend and that, in all age groups, violations of the maxims of Quantity and of Relation are sanctioned to a higher degree compared to infringements of the Maim of Manner and of the Maximize Presupposition principle. We conjecture that this relates to the effects that the violation of a certain maxim or principle has on the goals of the exchange: listeners are less tolerant with statements that transmit inaccurate or incomplete information, while being more tolerant with those that still permit to understand what has happened. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	journal = {Frontiers in Psychology},
	author = {Panzeri, Francesca and Foppolo, Francesca},
	year = {2021},
	keywords = {Age Differences, Audiences, Conversation, Judgment, Measurement, Pragmatics, Preschool Students, Primary School Students, Tolerance, Trends, Truth},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/TDB7KIUX/Panzeri and Foppolo - 2021 - Children’s and adults’ sensitivity to Gricean maxims and to the maximize presupposition principle.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/UF9BIRRX/2021-28482-001.html:text/html},
}

@inproceedings{rottger_two_2022-3,
	address = {Seattle, United States},
	title = {Two {Contrasting} {Data} {Annotation} {Paradigms} for {Subjective} {NLP} {Tasks}},
	url = {https://aclanthology.org/2022.naacl-main.13},
	doi = {10.18653/v1/2022.naacl-main.13},
	abstract = {Labelled data is the foundation of most natural language processing tasks. However, labelling data is difﬁcult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss beneﬁts and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.},
	language = {en},
	urldate = {2025-07-16},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Röttger, Paul and Vidgen, Bertie and Hovy, Dirk and Pierrehumbert, Janet},
	year = {2022},
	file = {PDF:/Users/carolinewagner/Zotero/storage/EN237I5R/Rottger et al. - 2022 - Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks.pdf:application/pdf},
}

@misc{laux_improving_2023-3,
	title = {Improving {Task} {Instructions} for {Data} {Annotators}: {How} {Clear} {Rules} and {Higher} {Pay} {Increase} {Performance} in {Data} {Annotation} in the {AI} {Economy}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Improving {Task} {Instructions} for {Data} {Annotators}},
	url = {https://arxiv.org/abs/2312.14565},
	abstract = {The global surge in AI applications is transforming industries, leading to displacement and complementation of existing jobs, while also giving rise to new employment opportunities. Data annotation, encompassing the labelling of images or annotating of texts by human workers, crucially influences the quality of a dataset directly influences the quality of AI models trained on it. This paper delves into the economics of data annotation, with a specific focus on the impact of task instruction design (that is, the choice between rules and standards as theorised in law and economics) and monetary incentives on data quality and costs. An experimental study involving 307 data annotators examines six groups with varying task instructions (norms) and monetary incentives. Results reveal that annotators provided with clear rules exhibit higher accuracy rates, outperforming those with vague standards by 14\%. Similarly, annotators receiving an additional monetary incentive perform significantly better, with the highest accuracy rate recorded in the group working with both clear rules and incentives (87.5\% accuracy). In addition, our results show that rules are perceived as being more helpful by annotators than standards and reduce annotators' difficulty in annotating images. These empirical findings underscore the double benefit of rule-based instructions on both data quality and worker wellbeing. Our research design allows us to reveal that, in our study, rules are more cost-efficient in increasing accuracy than monetary incentives. The paper contributes experimental insights to discussions on the economical, ethical, and legal considerations of AI technologies. Addressing policymakers and practitioners, we emphasise the need for a balanced approach in optimising data annotation processes for efficient and ethical AI development and usage.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Laux, Johann and Stephany, Fabian and Liefgreen, Alice},
	year = {2023},
	doi = {10.48550/ARXIV.2312.14565},
	keywords = {Applications (stat.AP), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Economics and business, General Economics (econ.GN)},
}

@book{grice_studies_1991-3,
	address = {Cambridge (Mass.) London},
	title = {Studies in the way of words},
	isbn = {978-0-674-85271-6},
	language = {eng},
	publisher = {Harvard university press},
	author = {Grice, H. Paul},
	year = {1991},
}

@article{kasirzadeh_conversation_2023-4,
	title = {In {Conversation} with {Artificial} {Intelligence}: {Aligning} language {Models} with {Human} {Values}},
	volume = {36},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2210-5433, 2210-5441},
	shorttitle = {In {Conversation} with {Artificial} {Intelligence}},
	url = {https://link.springer.com/10.1007/s13347-023-00606-x},
	doi = {10.1007/s13347-023-00606-x},
	abstract = {AbstractLarge-scale language technologies are increasingly used in various forms of communication with humans across different contexts. One particular use case for these technologies is conversational agents, which output natural language text in response to prompts and queries. This mode of engagement raises a number of social and ethical questions. For example, what does it mean to align conversational agents with human norms or values? Which norms or values should they be aligned with? And how can this be accomplished? In this paper, we propose a number of steps that help answer these questions. We start by developing a philosophical analysis of the building blocks of linguistic communication between conversational agents and human interlocutors. We then use this analysis to identify and formulate ideal norms of conversation that can govern successful linguistic communication between humans and conversational agents. Furthermore, we explore how these norms can be used to align conversational agents with human values across a range of different discursive domains. We conclude by discussing the practical implications of our proposal for the design of conversational agents that are aligned with these norms and values.},
	language = {en},
	number = {2},
	urldate = {2025-07-16},
	journal = {Philosophy \& Technology},
	author = {Kasirzadeh, Atoosa and Gabriel, Iason},
	month = jun,
	year = {2023},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/3558NHXZ/Kasirzadeh and Gabriel - 2023 - In Conversation with Artificial Intelligence Aligning language Models with Human Values.pdf:application/pdf},
}

@misc{kim_applying_2025-3,
	title = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}: {Design} {Insights} from a {Participatory} {Approach}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {Applying the {Gricean} {Maxims} to a {Human}-{LLM} {Interaction} {Cycle}},
	url = {https://arxiv.org/abs/2503.00858},
	abstract = {While large language models (LLMs) are increasingly used to assist users in various tasks through natural language interactions, these interactions often fall short due to LLMs' limited ability to infer contextual nuances and user intentions, unlike humans. To address this challenge, we draw inspiration from the Gricean Maxims–human communication theory that suggests principles of effective communication–and aim to derive design insights for enhancing human-AI interactions (HAI). Through participatory design workshops with communication experts, designers, and end-users, we identified ways to apply these maxims across the stages of the HAI cycle. Our findings include reinterpreted maxims tailored to human-LLM contexts and nine actionable design considerations categorized by interaction stage. These insights provide a concrete framework for designing more cooperative and user-centered LLM-based systems, bridging theoretical foundations in communication with practical applications in HAI.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Kim, Yoonsu and Chin, Brandon and Son, Kihoon and Kim, Seoyoung and Kim, Juho},
	year = {2025},
	doi = {10.48550/ARXIV.2503.00858},
	keywords = {FOS: Computer and information sciences, Human-Computer Interaction (cs.HC)},
}

@misc{saad_gricean_2025-3,
	title = {Gricean {Norms} as a {Basis} for {Effective} {Collaboration}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2503.14484},
	abstract = {Effective human-AI collaboration hinges not only on the AI agent's ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks – common ground, relevance theory, and theory of mind – into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent's pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Saad, Fardin and Murukannaiah, Pradeep K. and Singh, Munindar P.},
	year = {2025},
	doi = {10.48550/ARXIV.2503.14484},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Multiagent Systems (cs.MA)},
}

@misc{luo_enhanced_2025-3,
	title = {Enhanced {Bloom}'s {Educational} {Taxonomy} for {Fostering} {Information} {Literacy} in the {Era} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2503.19434},
	abstract = {The advent of Large Language Models (LLMs) has profoundly transformed the paradigms of information retrieval and problem-solving, enabling students to access information acquisition more efficiently to support learning. However, there is currently a lack of standardized evaluation frameworks that guide learners in effectively leveraging LLMs. This paper proposes an LLM-driven Bloom's Educational Taxonomy that aims to recognize and evaluate students' information literacy (IL) with LLMs, and to formalize and guide students practice-based activities of using LLMs to solve complex problems. The framework delineates the IL corresponding to the cognitive abilities required to use LLM into two distinct stages: Exploration \&amp; Action and Creation \&amp; Metacognition. It further subdivides these into seven phases: Perceiving, Searching, Reasoning, Interacting, Evaluating, Organizing, and Curating. Through the case presentation, the analysis demonstrates the framework's applicability and feasibility, supporting its role in fostering IL among students with varying levels of prior knowledge. This framework fills the existing gap in the analysis of LLM usage frameworks and provides theoretical support for guiding learners to improve IL.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Luo, Yiming and Liu, Ting and Pang, Patrick Cheong-Iao and McKay, Dana and Chen, Ziqi and Buchanan, George and Chang, Shanton},
	year = {2025},
	doi = {10.48550/ARXIV.2503.19434},
	keywords = {FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@misc{yaacoub_assessing_2025-3,
	title = {Assessing {AI}-{Generated} {Questions}' {Alignment} with {Cognitive} {Frameworks} in {Educational} {Assessment}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2504.14232},
	abstract = {This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz, an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured framework for categorizing educational objectives into hierarchical cognitive levels. Our research investigates whether incorporating this taxonomy can improve the alignment of AI-generated questions with specific cognitive objectives. We developed a dataset of 3691 questions categorized according to Bloom's levels and employed various classification models-Multinomial Logistic Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a Transformer-based model (DistilBERT)-to evaluate their effectiveness in categorizing questions. Our results indicate that higher Bloom's levels generally correlate with increased question length, Flesch-Kincaid Grade Level (FKGL), and Lexical Density (LD), reflecting the increased complexity of higher cognitive demands. Multinomial Logistic Regression showed varying accuracy across Bloom's levels, performing best for "Knowledge" and less accurately for higher-order levels. Merging higher-level categories improved accuracy for complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective classification for lower levels but struggled with higher-order tasks. DistilBERT achieved the highest performance, significantly improving classification of both lower and higher-order cognitive levels, achieving an overall validation accuracy of 91\%. This study highlights the potential of integrating Bloom's Taxonomy into AI-driven assessment tools and underscores the advantages of advanced models like DistilBERT for enhancing educational content generation.},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Yaacoub, Antoun and Da-Rugna, Jérôme and Assaghir, Zainab},
	year = {2025},
	doi = {10.48550/ARXIV.2504.14232},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{elim_promoting_2024-3,
	title = {Promoting cognitive skills in {AI}-supported learning environments: the integration of bloom’s taxonomy},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {0300-4279, 1475-7575},
	shorttitle = {Promoting cognitive skills in {AI}-supported learning environments},
	url = {https://www.tandfonline.com/doi/full/10.1080/03004279.2024.2332469},
	doi = {10.1080/03004279.2024.2332469},
	language = {en},
	urldate = {2025-07-16},
	journal = {Education 3-13},
	author = {Elim, Emily Hui Sein Yue},
	month = apr,
	year = {2024},
	pages = {1--11},
	file = {Full Text:/Users/carolinewagner/Zotero/storage/5ZXAAWC2/Elim - 2024 - Promoting cognitive skills in AI-supported learning environments the integration of bloom’s taxonom.pdf:application/pdf},
}

@article{hakkarainen_interrogative_2002-4,
	title = {The {Interrogative} {Model} of {Inquiry} and {Computer}-{Supported} {Collaborative} {Learning}},
	volume = {11},
	issn = {0926-7220},
	url = {http://link.springer.com/10.1023/A:1013076706416},
	doi = {10.1023/a:1013076706416},
	number = {1},
	urldate = {2025-07-16},
	journal = {Science and Education},
	author = {Hakkarainen, Kai and Sintonen, Matti},
	year = {2002},
	pages = {25--43},
}

@book{koralus_reason_2023-3,
	address = {Oxford},
	title = {Reason and {Inquiry}: {The} {Erotetic} {Theory}},
	isbn = {978-0-19-255701-8},
	shorttitle = {Reason and {Inquiry}},
	abstract = {Reason and Inquiry: The Erotetic Theory presents a unified theory of the human capacity for reasoning and decision-making. The book's central idea is that our minds naturally aim at resolving issues, and if we are sufficiently inquisitive in the process, we can avoid mistakes},
	language = {eng},
	publisher = {Oxford University Press, Incorporated},
	author = {Koralus, Philipp},
	year = {2023},
}

@misc{koralus_pyetr_2025-3,
	title = {{PyETR}},
	copyright = {© 2025 Philipp Koralus, Sean Moss, Mark Todd},
	publisher = {University of Oxford; University of Birmingham; Dreaming Spires},
	author = {Koralus, Philipp and Moss, Sean and Todd, Mark},
	year = {2025},
}

@article{kirk_prism_2024-3,
	title = {The {PRISM} {Alignment} {Dataset}: {What} {Participatory}, {Representative} and {Individualised} {Human} {Feedback} {Reveals} {About} the {Subjective} and {Multicultural} {Alignment} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {The {PRISM} {Alignment} {Dataset}},
	url = {https://arxiv.org/abs/2404.16019},
	doi = {10.48550/ARXIV.2404.16019},
	abstract = {Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.},
	urldate = {2025-07-16},
	author = {Kirk, Hannah Rose and Whitefield, Alexander and Röttger, Paul and Bean, Andrew and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
	year = {2024},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{devlin_bert_2018-3,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BERT}},
	url = {https://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{brachman_current_2025-1,
	title = {Current and {Future} {Use} of {Large} {Language} {Models} for {Knowledge} {Work}},
	url = {http://arxiv.org/abs/2503.16774},
	abstract = {Large Language Models (LLMs) have introduced a paradigm shift in interaction with AI technology, enabling knowledge workers to complete tasks by specifying their desired outcome in natural language. LLMs have the potential to increase productivity and reduce tedious tasks in an unprecedented way. A systematic study of LLM adoption for work can provide insight into how LLMs can best support these workers. To explore knowledge workers' current and desired usage of LLMs, we ran a survey (n=216). Workers described tasks they already used LLMs for, like generating code or improving text, but imagined a future with LLMs integrated into their workflows and data. We ran a second survey (n=107) a year later that validated our initial findings and provides insight into up-to-date LLM use by knowledge workers. We discuss implications for adoption and design of generative AI technologies for knowledge work.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Brachman, Michelle and El-Ashry, Amina and Dugan, Casey and Geyer, Werner},
	month = mar,
	year = {2025},
	doi = {10.48550/arXiv.2503.16774},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/9AP4HDTF/Brachman et al. - 2025 - Current and Future Use of Large Language Models for Knowledge Work.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/KNKBCH8U/2503.html:text/html},
}

@misc{chen_spiral_2024-1,
	title = {Spiral of {Silence}: {How} is {Large} {Language} {Model} {Killing} {Information} {Retrieval}? – {A} {Case} {Study} on {Open} {Domain} {Question} {Answering}},
	shorttitle = {Spiral of {Silence}},
	url = {http://arxiv.org/abs/2404.10496},
	abstract = {The practice of Retrieval-Augmented Generation (RAG), which integrates Large Language Models (LLMs) with retrieval systems, has become increasingly prevalent. However, the repercussions of LLM-derived content infiltrating the web and influencing the retrieval-generation feedback loop are largely uncharted territories. In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems. Taking the trending Open Domain Question Answering (ODQA) task as a point of entry, our findings reveal a potential digital "Spiral of Silence" effect, with LLM-generated text consistently outperforming human-authored content in search rankings, thereby diminishing the presence and impact of human contributions online. This trend risks creating an imbalanced information ecosystem, where the unchecked proliferation of erroneous LLM-generated content may result in the marginalization of accurate information. We urge the academic community to take heed of this potential issue, ensuring a diverse and authentic digital information landscape.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Chen, Xiaoyang and He, Ben and Lin, Hongyu and Han, Xianpei and Wang, Tianshu and Cao, Boxi and Sun, Le and Sun, Yingfei},
	month = jun,
	year = {2024},
	doi = {10.48550/arXiv.2404.10496},
	keywords = {Computer Science - Information Retrieval},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/3K9ZEGZ7/Chen et al. - 2024 - Spiral of Silence How is Large Language Model Killing Information Retrieval -- A Case Study on Ope.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/MTNHBPIN/2404.html:text/html},
}

@misc{weidinger_ethical_2021-1,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	doi = {10.48550/arXiv.2112.04359},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/DGNP4Q6H/Weidinger et al. - 2021 - Ethical and social risks of harm from Language Models.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/4HLANB9C/2112.html:text/html},
}

@misc{tamkin_understanding_2021-1,
	title = {Understanding the {Capabilities}, {Limitations}, and {Societal} {Impact} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2102.02503},
	abstract = {On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Tamkin, Alex and Brundage, Miles and Clark, Jack and Ganguli, Deep},
	month = feb,
	year = {2021},
	doi = {10.48550/arXiv.2102.02503},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/N7VAK2TN/Tamkin et al. - 2021 - Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/KY2FPD4Y/2102.html:text/html},
}

@incollection{koralus_decision_2022-1,
	edition = {1},
	title = {Decision and {Practical} {Reasoning}},
	isbn = {978-0-19-882376-6 978-0-19-186254-0},
	url = {https://academic.oup.com/book/45443/chapter/389466704},
	abstract = {Abstract This chapter extends the erotetic theory to decision-making, allowing the definition of a notion of an erotetic agent. The chapter diagnoses widely discussed decision-making phenomena like framing effects and the endowment effect as structurally similar to the reasoning fallacies discussed in previous chapters. The chapter obtains that, as in previous chapters, erotetic equilibrium secures classical standards of rationality, in this case classically rational choice. The chapter also considers how erotetic reasons-based decision-making can facilitate rapid action, making sense of what are sometimes termed “affordances.” Finally, the chapter considers unique threats the erotetic agent faces, and proposes an argument that deliberative democracy has particular legitimacy for erotetic agents: collective judgment at the end of a suitable deliberative process is a better reflection of group agency because it tends to be a better reflection of individual agency.},
	language = {en},
	urldate = {2025-07-30},
	booktitle = {Reason and {Inquiry}},
	publisher = {Oxford University PressOxford},
	author = {Koralus, Philipp and Moss, Sean},
	month = dec,
	year = {2022},
	doi = {10.1093/oso/9780198823766.003.0006},
	pages = {248--301},
	file = {PDF:/Users/carolinewagner/Zotero/storage/EX56N5HA/Koralus - 2022 - Decision and Practical Reasoning.pdf:application/pdf},
}

@misc{sclar_quantifying_2024-1,
	title = {Quantifying {Language} {Models}' {Sensitivity} to {Spurious} {Features} in {Prompt} {Design} or: {How} {I} learned to start worrying about prompt formatting},
	shorttitle = {Quantifying {Language} {Models}' {Sensitivity} to {Spurious} {Features} in {Prompt} {Design} or},
	url = {http://arxiv.org/abs/2310.11324},
	abstract = {As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.},
	urldate = {2025-07-30},
	publisher = {arXiv},
	author = {Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
	month = jul,
	year = {2024},
	doi = {10.48550/arXiv.2310.11324},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/MXWQT5UB/Sclar et al. - 2024 - Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or How I learned to.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/93669PWG/2310.html:text/html},
}

@misc{razavi_benchmarking_2025-1,
	title = {Benchmarking {Prompt} {Sensitivity} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2502.06065},
	abstract = {Large language Models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset PromptSET designed to investigate the effects of slight prompt variations on LLM performance. Using TriviaQA and HotpotQA datasets as the foundation of our work, we generate prompt variations and evaluate their effectiveness across multiple LLMs. We benchmark the prompt sensitivity prediction task employing state-of-the-art methods from related tasks, including LLM-based self-evaluation, text classification, and query performance prediction techniques. Our findings reveal that existing methods struggle to effectively address prompt sensitivity prediction, underscoring the need to understand how information needs should be phrased for accurate LLM responses.},
	urldate = {2025-07-30},
	publisher = {arXiv},
	author = {Razavi, Amirhossein and Soltangheis, Mina and Arabzadeh, Negar and Salamat, Sara and Zihayat, Morteza and Bagheri, Ebrahim},
	month = feb,
	year = {2025},
	doi = {10.48550/arXiv.2502.06065},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/E8J4546T/Razavi et al. - 2025 - Benchmarking Prompt Sensitivity in Large Language Models.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/9J5M5JJD/2502.html:text/html},
}

@article{jiang_how_2020-1,
	title = {How {Can} {We} {Know} {What} {Language} {Models} {Know}?},
	volume = {8},
	url = {https://aclanthology.org/2020.tacl-1.28/},
	doi = {10.1162/tacl_a_00324},
	abstract = {Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a \_\_ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a \_\_ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.},
	urldate = {2025-08-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
	editor = {Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2020},
	pages = {423--438},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/D4RX9Y3P/Jiang et al. - 2020 - How Can We Know What Language Models Know.pdf:application/pdf},
}

@inproceedings{press_measuring_2023-1,
	address = {Singapore},
	title = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.378/},
	doi = {10.18653/v1/2023.findings-emnlp.378},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	urldate = {2025-08-01},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah and Lewis, Mike},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {5687--5711},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/CU6QA6MD/Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap in Language Models.pdf:application/pdf},
}

@misc{li_large_2023-1,
	title = {Large {Language} {Models} {Understand} and {Can} be {Enhanced} by {Emotional} {Stimuli}},
	url = {http://arxiv.org/abs/2307.11760},
	abstract = {Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call "EmotionPrompt" that combines the original prompt with emotional stimuli), e.g., 8.00\% relative performance improvement in Instruction Induction and 115\% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9\% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.},
	urldate = {2025-08-01},
	publisher = {arXiv},
	author = {Li, Cheng and Wang, Jindong and Zhang, Yixuan and Zhu, Kaijie and Hou, Wenxin and Lian, Jianxun and Luo, Fang and Yang, Qiang and Xie, Xing},
	month = nov,
	year = {2023},
	doi = {10.48550/arXiv.2307.11760},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/WY399EGE/Li et al. - 2023 - Large Language Models Understand and Can be Enhanced by Emotional Stimuli.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/4E27ALPX/2307.html:text/html},
}

@article{gaber_evaluating_2025-2,
	title = {Evaluating large language model workflows in clinical decision support for triage and referral and diagnosis},
	volume = {8},
	copyright = {2025 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-025-01684-1},
	doi = {10.1038/s41746-025-01684-1},
	abstract = {Accurate medical decision-making is critical for both patients and clinicians. Patients often struggle to interpret their symptoms, determine their severity, and select the right specialist. Simultaneously, clinicians face challenges in integrating complex patient data to make timely, accurate diagnoses. Recent advances in large language models (LLMs) offer the potential to bridge this gap by supporting decision-making for both patients and healthcare providers. In this study, we benchmark multiple LLM versions and an LLM-based workflow incorporating retrieval-augmented generation (RAG) on a curated dataset of 2000 medical cases derived from the Medical Information Mart for Intensive Care database. Our findings show that these LLMs are capable of providing personalized insights into likely diagnoses, suggesting appropriate specialists, and assessing urgent care needs. These models may also support clinicians in refining diagnoses and decision-making, offering a promising approach to improving patient outcomes and streamlining healthcare delivery.},
	language = {en},
	number = {1},
	urldate = {2025-08-01},
	journal = {npj Digital Medicine},
	author = {Gaber, Farieda and Shaik, Maqsood and Allega, Fabio and Bilecz, Agnes Julia and Busch, Felix and Goon, Kelsey and Franke, Vedran and Akalin, Altuna},
	month = may,
	year = {2025},
	keywords = {Computational biology and bioinformatics, Diseases, Medical research, Signs and symptoms},
	pages = {263},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/Y6FPMBBQ/Gaber et al. - 2025 - Evaluating large language model workflows in clinical decision support for triage and referral and d.pdf:application/pdf},
}

@misc{ovadya_bridging_2023-1,
	title = {Bridging {Systems}: {Open} {Problems} for {Countering} {Destructive} {Divisiveness} across {Ranking}, {Recommenders}, and {Governance}},
	shorttitle = {Bridging {Systems}},
	url = {http://arxiv.org/abs/2301.09976},
	abstract = {Divisiveness appears to be increasing in much of the world, leading to concern about political violence and a decreasing capacity to collaboratively address large-scale societal challenges. In this working paper we aim to articulate an interdisciplinary research and practice area focused on what we call bridging systems: systems which increase mutual understanding and trust across divides, creating space for productive conflict, deliberation, or cooperation. We give examples of bridging systems across three domains: recommender systems on social media, collective response systems, and human-facilitated group deliberation. We argue that these examples can be more meaningfully understood as processes for attention-allocation (as opposed to "content distribution" or "amplification") and develop a corresponding framework to explore similarities - and opportunities for bridging - across these seemingly disparate domains. We focus particularly on the potential of bridging-based ranking to bring the benefits of offline bridging into spaces which are already governed by algorithms. Throughout, we suggest research directions that could improve our capacity to incorporate bridging into a world increasingly mediated by algorithms and artificial intelligence.},
	urldate = {2025-08-03},
	publisher = {arXiv},
	author = {Ovadya, Aviv and Thorburn, Luke},
	month = jul,
	year = {2023},
	doi = {10.48550/arXiv.2301.09976},
	keywords = {Computer Science - Social and Information Networks},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/E7P9GLII/Ovadya and Thorburn - 2023 - Bridging Systems Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/2IL43MBG/2301.html:text/html},
}

@misc{pearl_understanding_2013-1,
	title = {Understanding {Simpson}'s {Paradox}},
	url = {https://papers.ssrn.com/abstract=2343788},
	abstract = {Simpson's paradox is often presented as a compelling demonstration of why we need statistics education in our schools. It is a reminder of how easy it is to fall into a web of paradoxical conclusions when relying solely on intuition, unaided by rigorous statistical methods. In recent years, ironically, the paradox assumed an added dimension when educators began using it to demonstrate the limits of statistical methods, and why causal, rather than statistical considerations are necessary to avoid those paradoxical conclusions (Arah, 2008; Pearl, 2009, pp. 173-182; Wasserman, 2004).},
	language = {en},
	urldate = {2025-08-06},
	publisher = {Social Science Research Network},
	author = {Pearl, Judea},
	month = sep,
	year = {2013},
	doi = {10.2139/ssrn.2343788},
	note = {Place: Rochester, NY
Type: SSRN Scholarly Paper},
	keywords = {Judea Pearl, SSRN, Understanding Simpson's Paradox},
	file = {Submitted Version:/Users/carolinewagner/Zotero/storage/4DG92P3N/Pearl - 2013 - Understanding Simpson's Paradox.pdf:application/pdf},
}

@article{gaber_evaluating_2025-3,
	title = {Evaluating large language model workflows in clinical decision support for triage and referral and diagnosis},
	volume = {8},
	copyright = {2025 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-025-01684-1},
	doi = {10.1038/s41746-025-01684-1},
	abstract = {Accurate medical decision-making is critical for both patients and clinicians. Patients often struggle to interpret their symptoms, determine their severity, and select the right specialist. Simultaneously, clinicians face challenges in integrating complex patient data to make timely, accurate diagnoses. Recent advances in large language models (LLMs) offer the potential to bridge this gap by supporting decision-making for both patients and healthcare providers. In this study, we benchmark multiple LLM versions and an LLM-based workflow incorporating retrieval-augmented generation (RAG) on a curated dataset of 2000 medical cases derived from the Medical Information Mart for Intensive Care database. Our findings show that these LLMs are capable of providing personalized insights into likely diagnoses, suggesting appropriate specialists, and assessing urgent care needs. These models may also support clinicians in refining diagnoses and decision-making, offering a promising approach to improving patient outcomes and streamlining healthcare delivery.},
	language = {en},
	number = {1},
	urldate = {2025-08-10},
	journal = {npj Digital Medicine},
	author = {Gaber, Farieda and Shaik, Maqsood and Allega, Fabio and Bilecz, Agnes Julia and Busch, Felix and Goon, Kelsey and Franke, Vedran and Akalin, Altuna},
	month = may,
	year = {2025},
	keywords = {Computational biology and bioinformatics, Diseases, Medical research, Signs and symptoms},
	pages = {263},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/F8748GJ8/Gaber et al. - 2025 - Evaluating large language model workflows in clinical decision support for triage and referral and d.pdf:application/pdf},
}

@article{rydbeck_younger_2021-1,
	title = {Younger age at onset of colorectal cancer is associated with increased patient’s delay},
	volume = {154},
	issn = {0959-8049, 1879-0852},
	url = {https://www.ejcancer.com/article/S0959-8049(21)00395-6/fulltext},
	doi = {10.1016/j.ejca.2021.06.020},
	language = {English},
	urldate = {2025-08-10},
	journal = {European Journal of Cancer},
	author = {Rydbeck, Daniel and Asplund, Dan and Bock, David and Haglind, Eva and Park, Jennifer and Rosenberg, Jacob and Walming, Sofie and Angenete, Eva},
	month = sep,
	year = {2021},
	pmid = {34298377},
	keywords = {Age groups, Colorectal neoplasm, Delayed diagnosis, Symptoms},
	pages = {269--276},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/MV3MUTVB/Rydbeck et al. - 2021 - Younger age at onset of colorectal cancer is associated with increased patient’s delay.pdf:application/pdf},
}

@article{caluwaerts_deliberation_2023-1,
	title = {Deliberation and polarization: a multi-disciplinary review},
	volume = {5},
	issn = {2673-3145},
	shorttitle = {Deliberation and polarization},
	url = {https://www.frontiersin.org/journals/political-science/articles/10.3389/fpos.2023.1127372/full},
	doi = {10.3389/fpos.2023.1127372},
	abstract = {In recent years, deliberative democracy has drawn attention as a potential way of fighting polarization. Allowing citizens to exchange arguments and viewpoints on political issues in group, can have strong conflict-mitigating effects: it can foster opinion changes (thereby overcoming idea-based polarization), and improve relations between diametrically opposed groups (thereby tackling affective forms of polarization, such as affective polarization). However, these results conflict with social psychological and communication studies which find that communicative encounters between groups can lead to further polarization and even group think. The question therefore arises under which conditions deliberative interactions between citizens can decrease polarization. Based on a multidisciplinary systematic review of the literature, which includes a wide diversity of communicative encounters ranging from short classroom discussions to multi-weekend citizen assemblies, this paper reports several findings. First, we argue that the effects of communicative encounters on polarization are conditional on how those types of communication were conceptualized across disciplines. More precisely, we find depolarizing effects when group discussions adhere to a deliberative democracy framework, and polarizing effects when they do not. Second we find that the depolarizing effects depend on several design factors that are often implemented in deliberative democracy studies. Finally, our analysis shows that that much more work needs to be done to unravel and test the exact causal mechanism(s) underlying the polarization-reducing effects of deliberation. Many potential causal mechanisms were identified, but few studies were able to adjudicate how deliberation affects polarization.},
	language = {English},
	urldate = {2025-08-10},
	journal = {Frontiers in Political Science},
	author = {Caluwaerts, Didier and Bernaerts, Kamil and Kesberg, Rebekka and Smets, Lien and Spruyt, Bram},
	month = jun,
	year = {2023},
	keywords = {Affective polarization, deliberative democracy, Democracy, polarization, Systematic reveiw},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/BCBLP8IE/Caluwaerts et al. - 2023 - Deliberation and polarization a multi-disciplinary review.pdf:application/pdf},
}

@misc{noauthor_generically_nodate-1,
	title = {Generically partisan: {Polarization} in political communication},
	shorttitle = {Generically partisan},
	url = {https://www.pnas.org/doi/epdf/10.1073/pnas.2309361120},
	language = {en},
	urldate = {2025-08-10},
	doi = {10.1073/pnas.2309361120},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/HZGK4EAC/Generically partisan Polarization in political communication.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/XAJIXVC2/pnas.html:text/html},
}

@article{stewart_crowdsourcing_2017-1,
	title = {Crowdsourcing {Samples} in {Cognitive} {Science}},
	volume = {21},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661317301316},
	doi = {10.1016/j.tics.2017.06.007},
	abstract = {Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.},
	number = {10},
	urldate = {2025-08-10},
	journal = {Trends in Cognitive Sciences},
	author = {Stewart, Neil and Chandler, Jesse and Paolacci, Gabriele},
	month = oct,
	year = {2017},
	pages = {736--748},
	file = {ScienceDirect Full Text PDF:/Users/carolinewagner/Zotero/storage/TN5MASTH/Stewart et al. - 2017 - Crowdsourcing Samples in Cognitive Science.pdf:application/pdf;ScienceDirect Snapshot:/Users/carolinewagner/Zotero/storage/ST85QLBV/S1364661317301316.html:text/html},
}

@article{kyrychenko_profiling_2025-2,
	title = {Profiling misinformation susceptibility},
	volume = {241},
	issn = {0191-8869},
	url = {https://www.sciencedirect.com/science/article/pii/S0191886925001394},
	doi = {10.1016/j.paid.2025.113177},
	abstract = {The global spread of misinformation poses a serious threat to the functioning of societies worldwide. But who falls for it? In this study, 66,242 individuals from 24 countries completed the Misinformation Susceptibility Test (MIST) and indicated their self-perceived misinformation discernment ability. Multilevel modelling showed that Generation Z, non-male, less educated, and more conservative individuals were more vulnerable to misinformation. Furthermore, while individuals' confidence in detecting misinformation was generally associated with better actual discernment, the degree to which perceived ability matched actual ability varied across subgroups. That is, whereas women were especially accurate in assessing their ability, extreme conservatives' perceived ability showed little relation to their actual misinformation discernment. Meanwhile, across all generations, Gen Z perceived their misinformation discernment ability most accurately, despite performing worst on the test. Taken together, our analyses provide the first systematic and holistic profile of misinformation susceptibility.},
	urldate = {2025-08-10},
	journal = {Personality and Individual Differences},
	author = {Kyrychenko, Yara and Koo, Hyunjin J. and Maertens, Rakoen and Roozenbeek, Jon and van der Linden, Sander and Götz, Friedrich M.},
	month = jul,
	year = {2025},
	keywords = {Conspiracy theories, Fake news, Misinformation susceptibility, MIST},
	pages = {113177},
}

@misc{noauthor_design-based_nodate-1,
	title = {Design-based {Supervised} {Learning}},
	url = {https://naokiegami.com/dsl/},
	abstract = {R package dsl implements design-based supervised learning (DSL) proposed in Egami, Hinck, Stewart, and Wei (2023). DSL is a general estimation framework for using predicted variables in statistical analyses. The package is especially useful for researchers trying to use large language models (LLMs) to annotate a large number of documents they analyze subsequently. dsl allows users to obtain statistically valid estimates and standard errors, even when LLM annotations contain arbitrary non-random prediction errors and biases.},
	language = {en},
	urldate = {2025-08-10},
	file = {Snapshot:/Users/carolinewagner/Zotero/storage/N574S34P/dsl.html:text/html},
}

@article{kasirzadeh_conversation_2023-5,
	title = {In {Conversation} with {Artificial} {Intelligence}: {Aligning} language {Models} with {Human} {Values}},
	volume = {36},
	issn = {2210-5441},
	shorttitle = {In {Conversation} with {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s13347-023-00606-x},
	doi = {10.1007/s13347-023-00606-x},
	abstract = {Large-scale language technologies are increasingly used in various forms of communication with humans across different contexts. One particular use case for these technologies is conversational agents, which output natural language text in response to prompts and queries. This mode of engagement raises a number of social and ethical questions. For example, what does it mean to align conversational agents with human norms or values? Which norms or values should they be aligned with? And how can this be accomplished? In this paper, we propose a number of steps that help answer these questions. We start by developing a philosophical analysis of the building blocks of linguistic communication between conversational agents and human interlocutors. We then use this analysis to identify and formulate ideal norms of conversation that can govern successful linguistic communication between humans and conversational agents. Furthermore, we explore how these norms can be used to align conversational agents with human values across a range of different discursive domains. We conclude by discussing the practical implications of our proposal for the design of conversational agents that are aligned with these norms and values.},
	language = {en},
	number = {2},
	urldate = {2025-08-11},
	journal = {Philosophy \& Technology},
	author = {Kasirzadeh, Atoosa and Gabriel, Iason},
	month = apr,
	year = {2023},
	keywords = {Artificial intelligence, Conversational agents, Ethics of language models, Language technologies, Large language models, Value alignment},
	pages = {27},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/8GR5Q52T/Kasirzadeh and Gabriel - 2023 - In Conversation with Artificial Intelligence Aligning language Models with Human Values.pdf:application/pdf},
}

@techreport{bick_rapid_2024-2,
	title = {The {Rapid} {Adoption} of {Generative} {AI}},
	url = {https://s3.amazonaws.com/real.stlouisfed.org/wp/2024/2024-027.pdf},
	abstract = {Generative artificial intelligence (AI) is a potentially important new technology, but its impact on the economy depends on the speed and intensity of adoption. This paper reports results from a series of nationally representative U.S. surveys of generative AI use at work and at home. As of late 2024, nearly 40\% of the U.S. population age 18-64 uses generative AI. Among employed respondents, 23\% used generative AI for work at least once in the previous week: 9\% used it every workday, and 14\% on some but not all workdays. Relative to each technology’s first mass-market product launch, work adoption of generative AI has been as fast as the personal computer (PC), and overall adoption has been faster than either PCs or the internet. Generative AI and PCs have very similar early work adoption patterns by education, occupation, and other characteristics. Between 1 and 5\% of all work hours are currently assisted by generative AI, and respondents report time savings equivalent to 1.4\% of total work hours. This suggests that substantial productivity gains from generative AI are possible.},
	language = {en},
	urldate = {2025-08-11},
	author = {Bick, Alexander and Blandin, Adam and Deming, David J.},
	year = {2024},
	doi = {10.20955/wp.2024.027},
	file = {PDF:/Users/carolinewagner/Zotero/storage/5S9DSL6W/Bick et al. - 2024 - The Rapid Adoption of Generative AI.pdf:application/pdf},
}

@misc{bick_rapid_2024-3,
	title = {The {Rapid} {Adoption} of {Generative} {AI}},
	url = {https://www.nber.org/papers/w32966},
	abstract = {Generative artificial intelligence (AI) is a potentially important new technology, but its impact on the economy depends on the speed and intensity of adoption. This paper reports results from a series of nationally representative U.S. surveys of generative AI use at work and at home. As of late 2024, nearly 40 percent of the U.S. population age 18-64 uses generative AI. 23 percent of employed respondents had used generative AI for work at least once in the previous week, and 9 percent used it every work day. Relative to each technology’s first mass-market product launch, work adoption of generative AI has been as fast as the personal computer (PC), and overall adoption has been faster than either PCs or the internet. Generative AI and PCs have very similar early adoption patterns by education, occupation, and other characteristics. Between 1 and 5 percent of all work hours are currently assisted by generative AI, and respondents report time savings equivalent to 1.4 percent of total work hours. This suggests that substantial productivity gains from generative AI are possible.},
	urldate = {2025-08-11},
	publisher = {National Bureau of Economic Research},
	author = {Bick, Alexander and Blandin, Adam and Deming, David J.},
	month = sep,
	year = {2024},
	doi = {10.3386/w32966},
	doi = {10.3386/w32966},
	note = {Series: Working Paper Series
Type: Working Paper},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/LHVX95D6/Bick et al. - 2024 - The Rapid Adoption of Generative AI.pdf:application/pdf},
}

@article{thapa_large_2025-1,
	title = {Large language models ({LLM}) in computational social science: prospects, current state, and challenges},
	volume = {15},
	issn = {1869-5469},
	shorttitle = {Large language models ({LLM}) in computational social science},
	url = {https://doi.org/10.1007/s13278-025-01428-9},
	doi = {10.1007/s13278-025-01428-9},
	abstract = {The advent of large language models (LLMs) has marked a new era in the transformation of computational social science (CSS). This paper dives into the role of LLMs in CSS, particularly exploring their potential to revolutionize data analysis and content generation and contribute to a broader understanding of social phenomena. We begin by discussing the applications of LLMs in various computational problems in social science including sentiment analysis, hate speech detection, stance and humor detection, misinformation detection, event understanding, and social network analysis, illustrating their capacity to generate nuanced insights into human behavior and societal trends. Furthermore, we explore the innovative use of LLMs in generating social media content. We also discuss the various ethical, technical, and legal issues these applications pose, and considerations required for responsible LLM usage. We further present the challenges associated with data bias, privacy, and the integration of these models into existing research frameworks. This paper aims to provide a solid background on the potential of LLMs in CSS, their past applications, current problems, and how they can pave the way for revolutionizing CSS.},
	language = {en},
	number = {1},
	urldate = {2025-08-11},
	journal = {Social Network Analysis and Mining},
	author = {Thapa, Surendrabikram and Shiwakoti, Shuvam and Shah, Siddhant Bikram and Adhikari, Surabhi and Veeramani, Hariram and Nasim, Mehwish and Naseem, Usman},
	month = mar,
	year = {2025},
	keywords = {Computational social science, Large language models, Natural language processing, Social network analysis, Social science},
	pages = {4},
	file = {Full Text PDF:/Users/carolinewagner/Zotero/storage/T99DFISH/Thapa et al. - 2025 - Large language models (LLM) in computational social science prospects, current state, and challenge.pdf:application/pdf},
}

@misc{noauthor_get_nodate-1,
	title = {Get {Started} with dsl},
	url = {https://naokiegami.com/dsl/articles/intro.html},
	language = {en},
	urldate = {2025-08-12},
	file = {Snapshot:/Users/carolinewagner/Zotero/storage/8D5N33S7/intro.html:text/html},
}

@misc{pavlovic_understanding_2025-1,
	title = {Understanding {Model} {Calibration} – {A} gentle introduction and visual exploration of calibration and the expected calibration error ({ECE})},
	url = {http://arxiv.org/abs/2501.19047},
	abstract = {To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.},
	urldate = {2025-08-12},
	publisher = {arXiv},
	author = {Pavlovic, Maja},
	month = may,
	year = {2025},
	doi = {10.48550/arXiv.2501.19047},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {PDF:/Users/carolinewagner/Zotero/storage/X9ICQLHS/Pavlovic - 2025 - Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and t.pdf:application/pdf},
}

@misc{gawlikowski_survey_2022-1,
	title = {A {Survey} of {Uncertainty} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2107.03342},
	doi = {10.48550/arXiv.2107.03342},
	abstract = {Due to their increasing spread, confidence in neural network predictions became more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over or under confidence. Many researchers have been working on understanding and quantifying uncertainty in a neural network's prediction. As a result, different types and sources of uncertainty have been identified and a variety of approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. A comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and not reducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks, ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for the calibration of neural networks and give an overview of existing baselines and implementations. Different examples from the wide spectrum of challenges in different fields give an idea of the needs and challenges regarding uncertainties in practical applications. Additionally, the practical limitations of current methods for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao Xiang},
	month = jan,
	year = {2022},
	note = {arXiv:2107.03342 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/carolinewagner/Zotero/storage/FKZG7NNY/Gawlikowski et al. - 2022 - A Survey of Uncertainty in Deep Neural Networks.pdf:application/pdf;Snapshot:/Users/carolinewagner/Zotero/storage/SGTTYMFJ/2107.html:text/html},
}
